{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning (DQN) Tutorial\n",
    "=====================================\n",
    "**Author**: `Adam Paszke <https://github.com/apaszke>`_\n",
    "\n",
    "\n",
    "This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent\n",
    "on the CartPole-v0 task from the `OpenAI Gym <https://gym.openai.com/>`__.\n",
    "\n",
    "**Task**\n",
    "\n",
    "The agent has to decide between two actions - moving the cart left or\n",
    "right - so that the pole attached to it stays upright. You can find an\n",
    "official leaderboard with various algorithms and visualizations at the\n",
    "`Gym website <https://gym.openai.com/envs/CartPole-v0>`__.\n",
    "\n",
    ".. figure:: /_static/img/cartpole.gif\n",
    "   :alt: cartpole\n",
    "\n",
    "   cartpole\n",
    "\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action. In this\n",
    "task, rewards are +1 for every incremental timestep and the environment\n",
    "terminates if the pole falls over too far or the cart moves more then 2.4\n",
    "units away from center. This means better performing scenarios will run\n",
    "for longer duration, accumulating larger return.\n",
    "\n",
    "The CartPole task is designed so that the inputs to the agent are 4 real\n",
    "values representing the environment state (position, velocity, etc.).\n",
    "However, neural networks can solve the task purely by looking at the\n",
    "scene, so we'll use a patch of the screen centered on the cart as an\n",
    "input. Because of this, our results aren't directly comparable to the\n",
    "ones from the official leaderboard - our task is much harder.\n",
    "Unfortunately this does slow down the training, because we have to\n",
    "render all the frames.\n",
    "\n",
    "Strictly speaking, we will present the state as the difference between\n",
    "the current screen patch and the previous one. This will allow the agent\n",
    "to take the velocity of the pole into account from one image.\n",
    "\n",
    "**Packages**\n",
    "\n",
    "\n",
    "First, let's import needed packages. Firstly, we need\n",
    "`gym <https://gym.openai.com/docs>`__ for the environment\n",
    "(Install using `pip install gym`).\n",
    "We'll also use the following from PyTorch:\n",
    "\n",
    "-  neural networks (``torch.nn``)\n",
    "-  optimization (``torch.optim``)\n",
    "-  automatic differentiation (``torch.autograd``)\n",
    "-  utilities for vision tasks (``torchvision`` - `a separate\n",
    "   package <https://github.com/pytorch/vision>`__).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MLapeyrolerie/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment. It essentially maps (state, action) pairs\n",
    "   to their (next_state, reward) result, with the state being the\n",
    "   screen difference image as described later on.\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our model. But first, let quickly recap what a DQN is.\n",
    "\n",
    "DQN algorithm\n",
    "-------------\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is also known as the *return*. The discount,\n",
    "$\\gamma$, should be a constant between $0$ and $1$\n",
    "that ensures the sum converges. It makes rewards from the uncertain far\n",
    "future less important for our agent than the ones in the near future\n",
    "that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
    "\n",
    "To minimise this error, we will use the `Huber\n",
    "loss <https://en.wikipedia.org/wiki/Huber_loss>`__. The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy. We calculate\n",
    "this over a batch of transitions, $B$, sampled from the replay\n",
    "memory:\n",
    "\n",
    "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
    "\n",
    "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}\\end{align}\n",
    "\n",
    "Q-network\n",
    "^^^^^^^^^\n",
    "\n",
    "Our model will be a convolutional neural network that takes in the\n",
    "difference between the current and previous screen patches. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and\n",
    "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
    "network). In effect, the network is trying to predict the *expected return* of\n",
    "taking each action given the current input.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input extraction\n",
    "^^^^^^^^^^^^^^^^\n",
    "\n",
    "The code below are utilities for extracting and processing rendered\n",
    "images from the environment. It uses the ``torchvision`` package, which\n",
    "makes it easy to compose image transforms. Once you run the cell it will\n",
    "display an example patch that it extracted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "Hyperparameters and utilities\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "This cell instantiates our model and its optimizer, and defines some\n",
    "utilities:\n",
    "\n",
    "-  ``select_action`` - will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "-  ``plot_durations`` - a helper for plotting the durations of episodes,\n",
    "   along with an average over the last 100 episodes (the measure used in\n",
    "   the official evaluations). The plot will be underneath the cell\n",
    "   containing the main training loop, and will update after every\n",
    "   episode.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[-0.0678,  0.0482, -0.0147,  0.0638,  0.0689],\n",
       "           [-0.0973,  0.0860, -0.0334,  0.0922,  0.0577],\n",
       "           [-0.1064,  0.0893,  0.0623, -0.0240,  0.0198],\n",
       "           [-0.0512, -0.0228,  0.0437,  0.0911, -0.0250],\n",
       "           [-0.1090, -0.0599, -0.0356, -0.0599, -0.1133]],\n",
       " \n",
       "          [[ 0.0225, -0.0555,  0.1115,  0.0623, -0.0896],\n",
       "           [ 0.0966,  0.0805, -0.0776, -0.0187, -0.0660],\n",
       "           [-0.1084,  0.0912, -0.1055, -0.0656, -0.0216],\n",
       "           [ 0.0637,  0.0345, -0.0309,  0.0652, -0.0400],\n",
       "           [ 0.1075, -0.0618, -0.0308,  0.0059,  0.0902]],\n",
       " \n",
       "          [[ 0.0116, -0.0464,  0.1106,  0.0973,  0.1010],\n",
       "           [-0.0747, -0.1061, -0.0424, -0.0252,  0.0929],\n",
       "           [ 0.0084, -0.0433, -0.0932,  0.0147, -0.0455],\n",
       "           [-0.0411, -0.1103, -0.0683,  0.0565, -0.0686],\n",
       "           [-0.0698,  0.0797, -0.1052,  0.0822,  0.0241]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0479,  0.0432, -0.0820, -0.0469,  0.1014],\n",
       "           [-0.0784,  0.0272,  0.0674, -0.0910,  0.0398],\n",
       "           [ 0.0726, -0.0233, -0.0737, -0.0686,  0.0083],\n",
       "           [ 0.0296, -0.0873,  0.0949, -0.0985,  0.0579],\n",
       "           [ 0.0339,  0.0057, -0.0271,  0.0264,  0.0377]],\n",
       " \n",
       "          [[-0.0494, -0.0635, -0.0050, -0.0303, -0.0475],\n",
       "           [ 0.0171, -0.0918, -0.0020,  0.1025, -0.0160],\n",
       "           [-0.0656,  0.0486,  0.0801,  0.0770, -0.1087],\n",
       "           [-0.0754,  0.0854,  0.0418, -0.0824,  0.0763],\n",
       "           [ 0.0626,  0.1051,  0.0291,  0.0978, -0.0784]],\n",
       " \n",
       "          [[-0.0124, -0.0039, -0.0366, -0.0126,  0.0517],\n",
       "           [-0.1145, -0.0439,  0.0426, -0.0692, -0.0502],\n",
       "           [ 0.0872, -0.0519, -0.0318,  0.0004, -0.0733],\n",
       "           [-0.0468, -0.0146,  0.0132,  0.0255, -0.1063],\n",
       "           [ 0.0063,  0.0039,  0.0243, -0.0295,  0.0200]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0572, -0.0979,  0.0013, -0.1087,  0.0782],\n",
       "           [ 0.0578, -0.0688,  0.0648, -0.1047,  0.0409],\n",
       "           [-0.0241, -0.0232, -0.1059, -0.0010, -0.0304],\n",
       "           [ 0.0518,  0.0088, -0.0811,  0.0861, -0.0344],\n",
       "           [-0.0756, -0.0606, -0.0410, -0.0723,  0.1137]],\n",
       " \n",
       "          [[ 0.0660,  0.0443,  0.0406, -0.0794, -0.0765],\n",
       "           [-0.1051,  0.0304,  0.0356, -0.0328,  0.0062],\n",
       "           [ 0.0497, -0.0608,  0.1060,  0.0309,  0.0597],\n",
       "           [-0.0970, -0.1104,  0.0751, -0.0767, -0.0423],\n",
       "           [ 0.0818,  0.0398, -0.1099,  0.0373, -0.0915]],\n",
       " \n",
       "          [[ 0.0588, -0.0304, -0.0832,  0.0674, -0.0949],\n",
       "           [-0.0612, -0.0860,  0.0881, -0.0067, -0.0076],\n",
       "           [-0.0566, -0.0127, -0.0657,  0.0370, -0.0440],\n",
       "           [-0.0904, -0.0540, -0.0463, -0.1048,  0.0460],\n",
       "           [-0.0212,  0.0459,  0.0956,  0.0140,  0.0871]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0149, -0.0410, -0.0028, -0.0617, -0.0327],\n",
       "           [-0.0752,  0.0033,  0.1048,  0.0194,  0.0977],\n",
       "           [ 0.0474,  0.1135, -0.0040,  0.0571,  0.0213],\n",
       "           [-0.0609, -0.0940,  0.0114,  0.0748,  0.0750],\n",
       "           [-0.0870, -0.0413,  0.0593,  0.0196, -0.0583]],\n",
       " \n",
       "          [[ 0.0289,  0.0748,  0.0922, -0.0936, -0.0073],\n",
       "           [ 0.0806,  0.0744, -0.1137, -0.0015,  0.0942],\n",
       "           [ 0.0575,  0.0660,  0.0941,  0.0458, -0.0278],\n",
       "           [-0.0236,  0.0218, -0.0621,  0.0428, -0.0678],\n",
       "           [ 0.0853, -0.1054,  0.1145, -0.0282, -0.0246]],\n",
       " \n",
       "          [[-0.0245, -0.0209,  0.0678, -0.0207,  0.0488],\n",
       "           [-0.1112,  0.0630,  0.0757, -0.0612,  0.0208],\n",
       "           [-0.0937, -0.0842,  0.0371, -0.0436, -0.0231],\n",
       "           [-0.0499, -0.0122,  0.0271,  0.0380, -0.0623],\n",
       "           [ 0.0760, -0.0393, -0.0844, -0.0438,  0.0810]]],\n",
       " \n",
       " \n",
       "         [[[-0.1012,  0.1097, -0.0101,  0.0041, -0.0392],\n",
       "           [ 0.0931,  0.0749,  0.0091, -0.0993,  0.0535],\n",
       "           [-0.1128, -0.0377,  0.1132,  0.1026,  0.0491],\n",
       "           [-0.0727,  0.0358, -0.0279,  0.0313,  0.1144],\n",
       "           [-0.0297, -0.1102, -0.0342, -0.0017, -0.0706]],\n",
       " \n",
       "          [[-0.0765,  0.0401, -0.0871,  0.0425,  0.0895],\n",
       "           [ 0.0765,  0.0632,  0.1110,  0.0151, -0.0500],\n",
       "           [-0.0240,  0.0763, -0.0390, -0.1046, -0.0426],\n",
       "           [-0.0746, -0.0417,  0.0408,  0.0280,  0.0866],\n",
       "           [-0.1024,  0.0712,  0.0889, -0.0033, -0.0758]],\n",
       " \n",
       "          [[-0.1100,  0.1116,  0.0717,  0.0735, -0.0234],\n",
       "           [ 0.0794,  0.0231,  0.0394, -0.0990,  0.0789],\n",
       "           [ 0.0398, -0.0465, -0.0145,  0.0970, -0.1149],\n",
       "           [ 0.0395, -0.0514,  0.0087, -0.0194,  0.1115],\n",
       "           [ 0.0517,  0.0800,  0.0111, -0.0543, -0.0856]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0403,  0.0368,  0.0520,  0.0015,  0.0224],\n",
       "           [-0.0044, -0.0108, -0.0832,  0.0222,  0.0913],\n",
       "           [-0.0772,  0.1058, -0.0781, -0.0016,  0.1041],\n",
       "           [ 0.0563,  0.0797, -0.0613, -0.0077, -0.0542],\n",
       "           [-0.0920, -0.1114,  0.0595,  0.0650, -0.0528]],\n",
       " \n",
       "          [[-0.0675,  0.0564, -0.0723, -0.0261, -0.1021],\n",
       "           [ 0.0783,  0.0229,  0.0491,  0.0041,  0.0499],\n",
       "           [ 0.0206, -0.0319, -0.0754,  0.0337,  0.0753],\n",
       "           [-0.1000, -0.1041,  0.0660, -0.0788, -0.0736],\n",
       "           [-0.0247, -0.0804, -0.0398, -0.0917,  0.0141]],\n",
       " \n",
       "          [[ 0.0838,  0.0945, -0.1014,  0.0979, -0.1051],\n",
       "           [ 0.0922, -0.1068, -0.0708, -0.0205,  0.1006],\n",
       "           [-0.0266,  0.0183,  0.0333, -0.0288,  0.0244],\n",
       "           [-0.0362, -0.1047, -0.0125, -0.0749,  0.0489],\n",
       "           [-0.0284, -0.0313, -0.0806, -0.0341,  0.0205]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1009,  0.0779, -0.0705, -0.0224, -0.0812, -0.0555, -0.0512, -0.0791,\n",
       "          0.0581,  0.0408,  0.0624,  0.0837,  0.0880, -0.0911,  0.0222,  0.1111],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.8774e-02,  7.2616e-03,  1.0518e-02,  1.3728e-03,  2.9306e-02],\n",
       "           [ 2.7641e-02,  8.9132e-03, -9.2529e-04, -3.9568e-02,  2.1475e-02],\n",
       "           [ 7.7068e-03,  2.1176e-02,  1.3991e-02, -2.6778e-03,  1.1691e-02],\n",
       "           [ 1.6076e-03,  1.4464e-02, -1.8970e-02, -4.3441e-02,  4.8259e-02],\n",
       "           [-4.3274e-02,  3.0059e-02, -1.9117e-02,  3.1290e-02, -2.1616e-02]],\n",
       " \n",
       "          [[-1.4750e-02, -3.0821e-02,  1.0613e-02,  4.4407e-03,  3.2660e-02],\n",
       "           [-4.4822e-02, -4.8193e-03,  2.4932e-03, -2.7356e-02,  1.2460e-02],\n",
       "           [ 2.8425e-02,  1.4637e-03, -4.9220e-02, -2.7045e-02, -4.6305e-02],\n",
       "           [-2.2415e-02,  2.9001e-02, -1.4226e-03,  1.0955e-03,  2.7049e-02],\n",
       "           [-4.3969e-02,  2.8612e-02, -1.8378e-02, -1.3276e-02,  1.9248e-02]],\n",
       " \n",
       "          [[ 3.5328e-02,  1.6985e-03, -9.4878e-03,  2.9975e-02,  1.3595e-02],\n",
       "           [-6.9953e-03,  2.2191e-02, -3.0404e-02, -4.4698e-03,  2.3495e-03],\n",
       "           [ 3.8036e-04, -1.6555e-02, -1.1901e-02,  1.1900e-02, -2.8375e-02],\n",
       "           [-2.4900e-03,  1.3129e-02,  8.1443e-03,  2.7100e-02,  3.5613e-02],\n",
       "           [-2.0342e-04,  3.6364e-02, -2.6252e-02, -4.7057e-02, -4.4141e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.3137e-03,  3.9728e-02, -1.0828e-04,  2.2206e-02,  7.3540e-03],\n",
       "           [-2.3235e-02, -2.8776e-02,  2.1618e-02,  2.2692e-03, -4.4919e-03],\n",
       "           [ 8.6358e-03,  4.7924e-02,  2.4326e-02, -7.5804e-03, -1.8461e-02],\n",
       "           [ 4.7778e-03,  6.1831e-03,  3.1362e-04, -4.9580e-02,  4.1324e-02],\n",
       "           [ 1.5224e-02,  2.3743e-02,  2.5854e-02,  1.3191e-02,  2.3511e-02]],\n",
       " \n",
       "          [[-4.7801e-02, -6.9790e-03, -4.5724e-02, -1.8028e-02,  2.9041e-02],\n",
       "           [-7.4297e-05,  4.1525e-02,  4.1234e-03,  2.9355e-02,  2.7172e-02],\n",
       "           [-3.2544e-02, -4.7680e-03,  1.0924e-02,  6.9323e-03, -2.0163e-02],\n",
       "           [ 2.7239e-02,  4.1618e-02, -1.1395e-02,  2.3215e-02,  4.8437e-02],\n",
       "           [-8.1797e-03, -1.7249e-02, -1.6479e-02,  4.3625e-02, -1.1474e-02]],\n",
       " \n",
       "          [[-1.1177e-02, -1.9392e-02,  1.4767e-03, -4.9663e-02,  4.6542e-02],\n",
       "           [ 1.1676e-02, -4.1192e-02,  1.7413e-02, -1.6159e-02,  7.3582e-04],\n",
       "           [ 4.7928e-02,  2.9483e-02, -4.6779e-02,  2.3251e-02, -3.3662e-02],\n",
       "           [-9.9038e-04, -2.1496e-02,  1.6705e-02,  1.8198e-02, -6.3878e-05],\n",
       "           [-4.2510e-02, -3.1973e-02, -4.1717e-02, -3.7606e-02,  1.8497e-02]]],\n",
       " \n",
       " \n",
       "         [[[-9.9126e-03, -1.5270e-03, -1.4963e-02, -1.6638e-02,  4.8832e-02],\n",
       "           [-2.0766e-02, -1.4800e-02,  2.5464e-02,  3.8929e-02, -4.6627e-02],\n",
       "           [-4.7134e-02,  3.4586e-02, -4.1217e-03, -3.2842e-02, -4.0312e-02],\n",
       "           [ 4.2191e-02,  4.1894e-02,  5.1733e-03, -4.4286e-02, -4.6514e-02],\n",
       "           [ 1.8720e-02,  4.6606e-02,  8.6080e-03, -2.9089e-02, -4.6795e-02]],\n",
       " \n",
       "          [[-2.9579e-02, -3.9101e-02,  3.4187e-02,  4.0418e-02, -1.6697e-02],\n",
       "           [ 2.2583e-02,  4.6740e-02, -3.8036e-02,  4.4607e-02,  4.8601e-02],\n",
       "           [-1.9582e-02, -3.1887e-02, -3.5885e-02,  4.7520e-02,  1.0860e-02],\n",
       "           [-1.8261e-02,  4.8853e-02, -3.7095e-02,  2.3473e-02,  1.3269e-02],\n",
       "           [ 3.7438e-02, -1.0144e-02,  4.6594e-03,  2.6451e-02, -1.8132e-02]],\n",
       " \n",
       "          [[ 7.6725e-04,  5.1067e-03,  2.9433e-02, -2.3936e-03,  6.1814e-03],\n",
       "           [-2.9043e-03, -3.4715e-02,  8.5342e-03,  4.7314e-02,  1.5566e-02],\n",
       "           [-5.8262e-03,  4.2758e-02,  5.9613e-03,  1.2669e-02,  3.4707e-02],\n",
       "           [-3.9386e-02,  6.0100e-03,  1.7952e-02, -4.0335e-02,  2.1361e-02],\n",
       "           [ 4.5553e-02, -5.6452e-03, -2.8087e-02, -1.3849e-02,  1.0454e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.3700e-02,  3.3667e-02,  1.5291e-02, -3.5478e-02, -3.0661e-02],\n",
       "           [-3.5927e-02,  4.2790e-02, -2.8940e-02, -4.0831e-02,  3.3485e-02],\n",
       "           [ 1.4946e-02, -3.8435e-02,  1.0749e-02,  3.9451e-02,  4.2318e-02],\n",
       "           [ 2.0397e-02,  2.3437e-02,  3.9204e-02, -1.1135e-02,  1.6629e-02],\n",
       "           [-5.2985e-03,  3.5350e-02, -2.6248e-02,  3.1807e-02,  3.5275e-02]],\n",
       " \n",
       "          [[ 4.4703e-02,  2.9412e-02,  1.7908e-02, -1.6049e-02,  2.5652e-02],\n",
       "           [-3.6582e-02,  5.8967e-03,  7.4523e-03, -3.5612e-02, -2.2772e-02],\n",
       "           [-1.6414e-02, -4.5443e-02,  1.4630e-02,  3.0957e-02, -2.2990e-02],\n",
       "           [-4.8315e-02,  4.5348e-02, -3.9880e-02, -2.4350e-02,  1.7702e-02],\n",
       "           [ 4.3169e-02, -2.5312e-02,  4.9061e-02, -1.8978e-02,  4.2427e-02]],\n",
       " \n",
       "          [[-4.5625e-02, -3.1910e-02,  4.7023e-02, -1.0102e-02,  4.5618e-03],\n",
       "           [ 2.9610e-02,  2.4556e-02, -1.5558e-03, -1.3753e-02, -3.6885e-02],\n",
       "           [-1.2636e-02, -4.6862e-03,  4.6513e-02,  2.8806e-02, -4.5386e-02],\n",
       "           [ 4.9578e-02, -6.9543e-03,  5.3706e-03, -2.6409e-02, -2.8989e-02],\n",
       "           [ 3.3913e-02, -3.5167e-02,  4.2596e-02, -4.3566e-02,  2.6828e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.2285e-02, -2.1803e-02,  2.1781e-02, -1.5651e-02, -4.1547e-02],\n",
       "           [ 1.9621e-03, -3.1694e-02, -4.3411e-02,  3.3040e-02, -8.8824e-03],\n",
       "           [-3.1125e-02,  2.8746e-03,  4.1319e-02, -4.9477e-02, -4.9184e-02],\n",
       "           [-4.3126e-02, -4.7306e-02,  4.9309e-02, -2.2056e-02,  3.2387e-02],\n",
       "           [ 5.8794e-03,  3.6354e-02,  9.6338e-03,  3.2863e-02, -2.6095e-03]],\n",
       " \n",
       "          [[ 1.3289e-02, -3.4788e-02, -4.1371e-02, -4.4551e-02, -2.9359e-02],\n",
       "           [ 8.0842e-03, -3.8139e-02,  3.2769e-02,  3.2614e-02, -9.4776e-03],\n",
       "           [ 4.2918e-02,  3.0004e-02, -3.5131e-03,  7.0239e-03,  6.2371e-03],\n",
       "           [ 1.8847e-02,  2.1120e-02, -9.4995e-03, -1.6754e-02,  2.6524e-02],\n",
       "           [ 2.1694e-02, -4.3555e-02,  4.5566e-02,  4.9811e-02,  3.4072e-02]],\n",
       " \n",
       "          [[-1.5761e-02, -5.6382e-03, -2.4286e-02, -4.2683e-02,  3.8820e-02],\n",
       "           [-3.9698e-02,  4.6888e-02, -3.2279e-02, -4.1427e-02,  2.4720e-02],\n",
       "           [ 2.0954e-03,  1.4784e-02,  4.9193e-02, -1.3269e-02,  4.6302e-02],\n",
       "           [-4.5178e-02,  4.1440e-02,  5.7321e-03, -8.9201e-03, -3.8617e-02],\n",
       "           [-3.0571e-03, -4.3166e-02,  3.2856e-03,  3.9687e-02, -3.4958e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.4635e-03, -4.8868e-02,  3.5225e-02, -2.5360e-02, -1.7670e-02],\n",
       "           [-3.1870e-02, -2.1247e-02,  3.1399e-02,  4.2495e-02, -3.9562e-02],\n",
       "           [ 3.5425e-02, -5.5157e-03,  1.0029e-02, -1.2642e-02, -2.8227e-02],\n",
       "           [-4.3197e-02, -1.1956e-02, -4.2284e-02, -2.6276e-02,  4.8054e-02],\n",
       "           [ 4.7029e-02, -3.2879e-02,  4.5960e-03,  1.7245e-02,  2.1214e-02]],\n",
       " \n",
       "          [[ 2.1196e-02,  4.5539e-02, -8.4943e-03, -4.8743e-02, -4.1718e-02],\n",
       "           [ 4.4083e-02,  1.1966e-02, -1.3735e-02,  1.0441e-02, -2.9911e-02],\n",
       "           [-4.2264e-02,  1.4804e-02, -2.3178e-02, -3.7783e-02, -2.5726e-02],\n",
       "           [ 2.3785e-02,  1.1411e-02,  2.3472e-02, -3.2511e-02, -1.5395e-02],\n",
       "           [-2.1986e-03, -4.3620e-02, -6.5054e-03,  4.5453e-02,  4.3320e-02]],\n",
       " \n",
       "          [[-4.9273e-02, -1.4625e-02, -2.6059e-03, -4.4011e-02, -6.1850e-03],\n",
       "           [-2.7487e-02, -2.8794e-04, -6.7897e-03, -1.6207e-02,  1.2938e-02],\n",
       "           [-2.3474e-02,  4.7474e-02,  2.5388e-02,  4.4434e-02, -1.4260e-02],\n",
       "           [-4.0517e-02,  2.4846e-02, -1.4302e-02, -2.1044e-02, -1.6166e-02],\n",
       "           [ 1.4515e-03, -6.9820e-03,  1.5563e-02, -2.0204e-02,  4.5788e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 4.5847e-02,  2.7824e-02,  2.5862e-02,  1.2011e-02,  2.1969e-02],\n",
       "           [ 3.7693e-02, -1.9691e-02,  4.8893e-02,  3.1176e-02, -4.2802e-02],\n",
       "           [-4.2488e-02, -3.1020e-02, -3.5252e-02, -1.3837e-02,  3.0301e-02],\n",
       "           [ 9.7441e-03, -2.4805e-02, -2.9736e-02, -4.0426e-02,  4.9219e-02],\n",
       "           [ 2.2582e-02, -3.3227e-02, -3.6424e-02,  1.1312e-02, -4.5404e-02]],\n",
       " \n",
       "          [[ 3.7845e-02,  1.8977e-03,  3.1862e-02, -3.1881e-02, -3.7517e-02],\n",
       "           [-2.7976e-02, -4.6088e-02, -1.9814e-02, -3.2251e-02, -3.9762e-03],\n",
       "           [-3.7570e-02, -1.0289e-02, -2.4740e-02, -3.6404e-02,  4.4192e-02],\n",
       "           [ 3.4507e-02,  4.4527e-02, -1.5919e-02, -4.2120e-02,  3.6037e-02],\n",
       "           [ 4.4642e-02, -2.3697e-02, -1.3332e-02,  3.6779e-02, -3.1272e-02]],\n",
       " \n",
       "          [[-4.7948e-02, -2.5694e-02, -4.0732e-02, -4.9902e-02, -1.1406e-03],\n",
       "           [ 6.2161e-03,  4.4075e-02, -1.6217e-03,  2.4359e-02, -6.5095e-03],\n",
       "           [ 6.0536e-03,  4.6667e-02, -3.2967e-02, -4.0632e-02, -1.4204e-02],\n",
       "           [-2.2641e-02, -4.1127e-02, -7.2791e-03,  9.0602e-03, -4.6982e-02],\n",
       "           [-3.3157e-02,  2.5319e-02,  3.6159e-02,  2.7436e-02,  4.6712e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.8837e-02,  3.9215e-02, -2.2827e-02,  2.5907e-02,  3.3107e-02],\n",
       "           [ 2.9412e-02, -1.8790e-02,  1.8641e-02,  2.7404e-02, -1.3794e-02],\n",
       "           [ 6.2528e-03,  3.9243e-02, -4.7631e-02, -2.3993e-02,  2.5383e-02],\n",
       "           [ 8.2298e-03,  3.6739e-02,  3.9305e-02,  4.0158e-02, -1.4054e-02],\n",
       "           [-2.6949e-02, -4.1457e-02, -4.8720e-02,  2.5280e-02,  2.3914e-02]],\n",
       " \n",
       "          [[ 1.4692e-02, -1.6370e-02,  2.5768e-02,  2.5032e-02, -4.9279e-02],\n",
       "           [ 1.4332e-02, -4.5952e-02,  1.4907e-02,  1.4229e-02, -1.7824e-02],\n",
       "           [ 4.1090e-02,  1.4043e-02, -3.1310e-02, -4.5785e-02, -4.3162e-02],\n",
       "           [ 5.1223e-03, -2.1191e-02,  4.5921e-02,  4.6460e-03, -3.0292e-02],\n",
       "           [-3.8102e-02,  1.0036e-02, -1.2327e-02, -1.0183e-02, -4.7218e-02]],\n",
       " \n",
       "          [[-1.3529e-02, -1.9203e-03, -3.4271e-02, -1.4038e-02,  3.6646e-02],\n",
       "           [-2.5458e-02,  3.4842e-02,  4.6783e-02, -4.4327e-02, -2.6244e-02],\n",
       "           [ 2.8391e-02, -8.2287e-03, -4.2175e-02, -4.0536e-02,  2.3743e-02],\n",
       "           [ 3.2704e-02,  1.1014e-02,  1.8130e-03, -2.1679e-02,  1.6754e-02],\n",
       "           [ 3.3630e-03, -4.5676e-02,  4.2627e-02,  1.3468e-02, -4.2556e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.0699e-02, -9.2835e-03, -7.1056e-03, -4.1363e-02,  1.0358e-02],\n",
       "           [-1.0888e-02,  4.3268e-02,  1.1163e-02, -1.3842e-02, -6.9198e-03],\n",
       "           [ 1.7812e-02, -2.0636e-02,  1.6719e-02,  1.7967e-03, -4.9929e-02],\n",
       "           [-1.4463e-02, -4.4007e-02,  2.4493e-02, -4.3863e-02,  4.3538e-02],\n",
       "           [ 1.0760e-02, -1.1057e-02,  4.0553e-04, -3.3552e-04,  1.7138e-02]],\n",
       " \n",
       "          [[ 1.7041e-02, -7.7860e-03,  4.1532e-02,  2.1080e-03,  2.1833e-02],\n",
       "           [-1.7140e-02,  3.8230e-02, -1.8191e-02,  1.4306e-03, -1.1903e-02],\n",
       "           [ 3.0468e-04,  1.5025e-02, -4.4991e-02,  2.4632e-03, -4.8133e-02],\n",
       "           [ 2.6663e-02, -4.2251e-02, -5.7384e-03, -7.2447e-03,  1.6680e-03],\n",
       "           [-3.4420e-02,  4.7237e-02,  1.5042e-02, -2.0380e-02,  8.7490e-03]],\n",
       " \n",
       "          [[-2.4186e-02, -2.3877e-02,  4.0106e-02,  3.5447e-02, -2.5690e-02],\n",
       "           [-1.7942e-02,  1.8304e-02, -9.4686e-03, -4.2361e-02, -1.8420e-02],\n",
       "           [-1.1160e-02, -1.9868e-02,  1.5339e-02,  4.2627e-02, -3.1503e-02],\n",
       "           [-6.7830e-03,  4.1517e-02, -1.1869e-02, -2.0695e-02, -1.9079e-02],\n",
       "           [-1.6389e-02,  4.4409e-02, -1.3892e-02, -3.1310e-02, -3.6826e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.9257e-03,  1.3099e-03, -2.6700e-02,  2.7711e-02,  4.3702e-02],\n",
       "           [-4.5453e-02, -3.1347e-02, -1.8856e-02,  3.7929e-02, -3.8954e-02],\n",
       "           [ 1.3494e-02, -8.2981e-03,  4.1888e-02,  3.7041e-02,  3.8080e-02],\n",
       "           [ 4.0621e-03, -1.7772e-02, -3.0944e-02, -3.8738e-02, -9.9254e-03],\n",
       "           [-2.3129e-02, -4.7000e-02, -1.6586e-03,  3.7007e-02,  3.7608e-02]],\n",
       " \n",
       "          [[ 8.3374e-03, -8.1316e-03,  2.8498e-02,  4.6673e-02,  4.9501e-02],\n",
       "           [-1.4493e-02,  2.8142e-02,  2.9713e-02, -2.6120e-02, -3.4239e-02],\n",
       "           [ 7.8907e-03,  2.6826e-02,  4.7159e-02, -1.4580e-02, -4.6089e-02],\n",
       "           [-3.1673e-02,  3.6429e-02,  2.2957e-02, -2.8302e-02,  6.7841e-03],\n",
       "           [-6.3284e-03,  1.8134e-02,  4.7143e-02,  2.7541e-02, -4.1140e-02]],\n",
       " \n",
       "          [[-1.1005e-02,  1.4781e-02, -1.3720e-02,  3.8751e-02, -3.3098e-02],\n",
       "           [ 3.2105e-02,  2.2560e-02, -4.0502e-02, -4.7070e-02,  3.0096e-03],\n",
       "           [ 2.7930e-02,  4.5678e-02,  3.5900e-02,  1.0429e-02,  1.3527e-02],\n",
       "           [ 3.4228e-02, -5.0001e-03, -2.0910e-02, -1.6252e-02,  2.6513e-02],\n",
       "           [-2.3135e-02,  3.1634e-02,  2.5727e-02,  2.3714e-02,  1.5025e-03]]],\n",
       " \n",
       " \n",
       "         [[[-3.2851e-02, -1.3643e-02, -2.3614e-03,  4.9940e-02,  2.7697e-02],\n",
       "           [-3.3413e-02, -1.5426e-02, -2.2323e-02, -4.4309e-02, -7.9728e-03],\n",
       "           [-7.6057e-03,  2.3202e-02,  4.2853e-02, -2.1528e-02, -2.6856e-02],\n",
       "           [-3.1535e-02, -9.5690e-03,  3.2549e-03,  3.2198e-02, -4.8991e-04],\n",
       "           [-4.9200e-02,  2.0269e-02,  2.6573e-02,  4.9344e-02,  2.6099e-02]],\n",
       " \n",
       "          [[-3.1016e-02, -1.5287e-02,  3.1442e-02,  3.7307e-02,  4.2914e-02],\n",
       "           [ 2.3009e-02,  4.4641e-02, -3.2879e-02, -1.2716e-02,  3.5927e-02],\n",
       "           [ 3.6398e-02,  3.7651e-02,  2.5082e-02,  7.8852e-03, -9.9806e-03],\n",
       "           [ 1.0173e-02,  2.0716e-02,  3.4539e-03, -3.7034e-02, -1.5700e-02],\n",
       "           [ 1.6796e-02, -3.5205e-02, -3.2905e-02,  1.7833e-02,  2.6703e-02]],\n",
       " \n",
       "          [[ 2.2479e-02,  2.6066e-02,  4.3379e-02,  4.3342e-02, -3.8166e-03],\n",
       "           [ 4.5838e-02,  1.6756e-02, -1.8544e-02, -1.1982e-02, -1.4411e-02],\n",
       "           [-1.3901e-02, -2.5171e-02,  1.2825e-02, -3.6067e-02, -1.2633e-02],\n",
       "           [-2.6041e-02,  1.1308e-02,  2.0367e-02,  3.1792e-02, -2.5484e-02],\n",
       "           [-4.7742e-02,  4.6876e-02, -2.0276e-02,  4.4225e-02,  3.4700e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.9147e-02, -3.1481e-02, -2.8443e-02,  9.1464e-03,  9.5698e-03],\n",
       "           [-4.9895e-02, -4.4449e-02, -4.2136e-02,  4.1389e-02, -4.1880e-02],\n",
       "           [-4.3415e-02,  3.9935e-02,  9.7779e-03,  2.3615e-02, -4.0529e-02],\n",
       "           [ 5.5201e-03, -2.4619e-03,  3.4010e-02, -1.6725e-02,  1.2742e-03],\n",
       "           [ 9.5446e-03, -2.2811e-02, -3.1794e-02,  9.1152e-03, -1.6919e-02]],\n",
       " \n",
       "          [[ 5.4322e-03, -2.8293e-02, -3.7160e-02, -1.9457e-02, -4.0429e-02],\n",
       "           [ 4.9154e-02,  2.9381e-02, -2.9114e-02, -7.4434e-03, -1.5614e-02],\n",
       "           [ 4.9158e-02,  4.0965e-03, -9.4117e-03, -6.4038e-03,  2.8331e-02],\n",
       "           [-3.0660e-02, -2.8382e-02,  4.5929e-02, -2.9071e-02,  7.9610e-03],\n",
       "           [-2.3717e-02,  4.7179e-02,  7.7659e-03, -3.5368e-02, -4.4064e-02]],\n",
       " \n",
       "          [[-4.6857e-02,  2.6540e-02, -4.6097e-02,  4.7760e-02,  2.5843e-02],\n",
       "           [ 1.2723e-02,  3.3265e-02,  2.9663e-03, -4.3367e-02,  1.5783e-02],\n",
       "           [ 1.0465e-02,  1.2726e-04, -4.2687e-02,  1.6642e-02,  4.4115e-02],\n",
       "           [ 6.3094e-04, -4.3375e-02, -1.5637e-02, -3.4014e-02,  6.9444e-03],\n",
       "           [ 2.2604e-02, -3.8568e-02,  4.5588e-02,  2.6384e-02,  2.3046e-02]]]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0278,  0.0290, -0.0133,  0.0247, -0.0236,  0.0138, -0.0228, -0.0076,\n",
       "          0.0049, -0.0294, -0.0251,  0.0225,  0.0161, -0.0461,  0.0113,  0.0081,\n",
       "          0.0148,  0.0233, -0.0081, -0.0211,  0.0307,  0.0211,  0.0142,  0.0012,\n",
       "         -0.0413, -0.0091,  0.0140,  0.0217,  0.0074,  0.0020,  0.0310, -0.0075],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.1081e-02,  6.1058e-03, -1.0769e-02,  1.3592e-02,  2.5185e-03],\n",
       "           [-8.3692e-03, -2.6239e-02, -2.0195e-02, -1.2051e-02,  3.4937e-02],\n",
       "           [-4.7534e-03,  3.8155e-03,  7.8706e-03,  1.4410e-02,  1.9943e-02],\n",
       "           [-2.0217e-02,  1.3146e-02,  2.6301e-03,  8.2245e-03, -1.8791e-02],\n",
       "           [ 7.5663e-03,  3.4270e-02, -1.7947e-02,  1.3132e-02, -2.6324e-02]],\n",
       " \n",
       "          [[-9.0169e-03,  1.4821e-02,  1.2127e-02,  2.5149e-02, -2.9323e-02],\n",
       "           [ 4.7099e-03, -1.2095e-02, -2.5296e-02, -2.1319e-02, -2.5990e-02],\n",
       "           [-9.5445e-03, -2.0527e-02,  1.4003e-02, -1.9470e-02,  2.2555e-02],\n",
       "           [ 1.5009e-02, -2.4454e-02, -3.0599e-02,  2.9066e-02,  3.1786e-02],\n",
       "           [ 3.4949e-02, -1.8702e-02, -1.3668e-02,  1.0820e-02,  9.9584e-04]],\n",
       " \n",
       "          [[ 1.0668e-02, -1.7734e-02, -1.4092e-02,  1.4112e-02,  2.5699e-02],\n",
       "           [-1.7970e-02, -1.9554e-02, -2.2228e-02, -2.2903e-02,  9.3965e-03],\n",
       "           [-1.0285e-02,  3.0071e-02,  3.4553e-02, -2.0660e-02, -3.0383e-02],\n",
       "           [ 2.4607e-02,  7.8064e-03, -9.3352e-03, -1.4610e-02, -2.8279e-03],\n",
       "           [ 4.5032e-03, -2.1237e-02, -2.8572e-02,  2.7647e-02,  3.1691e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.2922e-03, -2.2742e-02, -3.3731e-02, -9.2118e-03, -2.2827e-02],\n",
       "           [-2.5168e-02, -3.4824e-02, -2.0389e-02, -2.9517e-02,  1.1984e-02],\n",
       "           [-2.7929e-02,  3.0347e-02,  3.1223e-02, -2.0599e-02,  3.4517e-02],\n",
       "           [ 2.0235e-02, -1.4968e-02, -2.8953e-02, -2.7530e-02, -2.4964e-03],\n",
       "           [ 9.1659e-03,  2.0421e-03, -3.1037e-02, -1.0591e-02,  4.2619e-03]],\n",
       " \n",
       "          [[-2.3169e-02,  1.2551e-02,  4.8733e-03, -1.7114e-02,  3.2554e-02],\n",
       "           [-8.1735e-03, -2.8430e-02, -1.4511e-03, -6.1665e-03, -8.4860e-03],\n",
       "           [-3.3167e-02,  1.4366e-02, -1.6338e-02, -3.2730e-02, -1.0659e-02],\n",
       "           [-1.5366e-03,  2.8453e-02,  1.4258e-02,  1.7786e-02, -1.9283e-02],\n",
       "           [ 1.9422e-02,  3.2680e-02,  3.1447e-02, -3.4609e-02, -2.5170e-02]],\n",
       " \n",
       "          [[ 2.2837e-02, -1.1501e-02, -2.8578e-02, -2.3257e-02, -5.8706e-03],\n",
       "           [ 2.7050e-02, -2.2881e-02,  2.0119e-02,  2.3945e-02, -3.2761e-02],\n",
       "           [ 3.4589e-02,  2.1863e-02, -1.2756e-02, -1.3455e-02, -2.9725e-02],\n",
       "           [-5.7404e-03, -2.2707e-02,  1.8044e-02, -2.5581e-03,  3.0356e-02],\n",
       "           [-1.9251e-02, -3.4155e-02,  6.4992e-03, -2.4946e-02,  6.7334e-03]]],\n",
       " \n",
       " \n",
       "         [[[-2.8856e-02, -1.4269e-02, -2.5743e-02,  5.2893e-03, -3.2447e-02],\n",
       "           [ 1.5535e-02,  3.1525e-02, -7.7161e-03, -3.0244e-02,  1.7800e-02],\n",
       "           [ 1.8463e-02,  1.3327e-02, -1.8756e-03,  3.3551e-02,  1.0215e-02],\n",
       "           [-4.9998e-04,  3.7173e-03,  1.1979e-02, -1.3937e-02,  2.4390e-02],\n",
       "           [-1.6691e-02,  8.2501e-03,  2.0594e-02,  2.6744e-03,  9.7774e-03]],\n",
       " \n",
       "          [[-3.1197e-04,  1.3339e-02, -1.3236e-02, -1.6682e-02,  2.9487e-02],\n",
       "           [ 1.2001e-03,  6.0103e-03, -3.2304e-02,  2.5430e-02,  2.3024e-02],\n",
       "           [ 2.6773e-02,  1.7774e-02, -7.6538e-03, -1.2131e-02, -2.4485e-02],\n",
       "           [-1.9679e-02,  6.1669e-04, -3.5471e-03, -8.4893e-03,  1.1998e-02],\n",
       "           [-2.7050e-02, -2.3647e-02,  1.7069e-02,  2.1082e-02, -1.8671e-02]],\n",
       " \n",
       "          [[-3.0660e-02, -1.9971e-02,  3.1970e-02,  1.7750e-02,  2.4776e-02],\n",
       "           [-1.8382e-02, -2.5330e-02,  3.5019e-02, -3.0430e-02,  2.3050e-02],\n",
       "           [-1.3945e-02,  1.7331e-02,  1.5635e-02,  1.0932e-03, -3.2859e-03],\n",
       "           [ 2.5459e-03, -3.5216e-02, -2.1817e-02,  3.2006e-02,  3.1008e-02],\n",
       "           [ 1.8628e-02,  2.1594e-02,  1.2287e-02, -3.2944e-02, -2.4019e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.6954e-02,  1.3387e-02,  1.2888e-02, -1.6127e-02,  1.0275e-02],\n",
       "           [ 6.5827e-04,  7.1951e-03,  4.3103e-03, -2.2193e-02, -1.9541e-02],\n",
       "           [ 1.8081e-02, -2.0584e-02, -8.3950e-03,  2.9828e-02, -2.2468e-02],\n",
       "           [-2.6880e-02, -2.7529e-02,  1.6681e-02,  6.8202e-03,  1.0241e-02],\n",
       "           [-2.9820e-02,  2.3004e-02,  1.3259e-02,  1.5773e-02, -9.0880e-03]],\n",
       " \n",
       "          [[-1.5466e-02, -3.4710e-02,  8.7359e-03,  2.1506e-02,  3.4264e-02],\n",
       "           [-2.4156e-02,  2.3094e-02,  3.1593e-02,  5.5947e-03, -1.9912e-02],\n",
       "           [-2.2104e-02,  2.6407e-02,  2.9032e-02, -1.1898e-03, -3.1872e-02],\n",
       "           [-1.5572e-02, -2.3401e-02, -2.1063e-02,  7.9365e-03, -4.5155e-03],\n",
       "           [ 3.4653e-02, -6.4257e-03, -2.9568e-02, -1.1316e-02, -2.2356e-02]],\n",
       " \n",
       "          [[-2.9418e-02,  2.7385e-02,  2.0743e-03, -2.9299e-02, -1.2112e-02],\n",
       "           [-1.5825e-02,  2.2945e-02,  3.4654e-02,  1.0762e-02,  1.5901e-02],\n",
       "           [-2.5087e-02, -3.0929e-02,  4.4059e-03, -3.0109e-02,  1.0906e-02],\n",
       "           [-2.8758e-02,  9.7710e-04, -2.3034e-02,  3.0063e-02,  2.3270e-02],\n",
       "           [ 3.2753e-02,  3.1017e-02, -1.9441e-02,  2.4343e-02, -8.7183e-03]]],\n",
       " \n",
       " \n",
       "         [[[-2.5747e-02,  6.4420e-03,  2.1593e-02,  3.1992e-02,  1.9252e-03],\n",
       "           [-1.4076e-03,  1.7607e-02, -1.9518e-02,  2.2049e-02, -1.6831e-03],\n",
       "           [ 1.8504e-02, -6.2456e-03,  1.2379e-02, -3.3078e-02,  1.7411e-02],\n",
       "           [ 3.3543e-02,  2.3800e-02, -1.5495e-02,  1.0851e-02, -1.8136e-02],\n",
       "           [ 1.5228e-02,  9.2190e-03,  4.4413e-03, -2.5569e-03,  2.0994e-02]],\n",
       " \n",
       "          [[-2.5111e-02,  1.5626e-02,  1.2110e-02,  1.9637e-02, -1.6338e-02],\n",
       "           [-1.2011e-02, -2.0914e-02,  3.1532e-02, -8.8110e-03,  1.5912e-02],\n",
       "           [-2.6768e-02,  4.2235e-03,  5.6798e-03,  3.4489e-02, -2.5004e-02],\n",
       "           [-9.7324e-03, -1.2508e-02, -3.2661e-02,  3.1524e-02, -3.4753e-02],\n",
       "           [-2.1958e-02, -8.1209e-03, -4.1754e-03, -1.2028e-02, -1.5148e-02]],\n",
       " \n",
       "          [[ 5.3531e-03, -2.6628e-03, -3.1453e-02, -1.5681e-03, -2.8092e-03],\n",
       "           [ 2.1983e-02, -3.0777e-02,  6.7161e-03,  2.7434e-03, -1.0848e-02],\n",
       "           [-7.5920e-03, -1.4691e-02, -3.2819e-02, -1.3826e-02, -2.6431e-02],\n",
       "           [-5.4577e-03, -3.5041e-02,  8.1001e-03,  3.0955e-02, -1.9831e-02],\n",
       "           [ 2.3429e-03, -1.7762e-04,  3.4877e-02, -5.2474e-03, -4.2546e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.2872e-02, -2.9925e-02,  6.7325e-03, -2.2885e-02, -1.5432e-02],\n",
       "           [ 7.6081e-03, -1.0134e-02, -2.2948e-02,  1.1294e-02,  2.5405e-02],\n",
       "           [-2.0494e-02, -3.0524e-02,  1.6117e-03, -2.9619e-02,  2.2813e-03],\n",
       "           [ 1.7356e-02, -3.4410e-02,  1.7937e-02,  1.1202e-02,  3.8959e-03],\n",
       "           [-3.5179e-02,  2.1614e-02, -1.6824e-03,  2.8112e-02, -1.5187e-02]],\n",
       " \n",
       "          [[ 5.2910e-04,  1.2764e-02, -8.4714e-03,  3.4111e-02, -1.9581e-02],\n",
       "           [ 3.1287e-02, -4.8232e-03, -6.7660e-03, -2.0970e-02, -6.5913e-03],\n",
       "           [ 2.7666e-02, -2.1453e-02,  2.5318e-02, -7.3988e-03,  2.1044e-02],\n",
       "           [ 2.0006e-02,  2.8512e-02,  8.4023e-03,  8.2841e-03, -2.8415e-02],\n",
       "           [ 1.5595e-02,  2.8610e-02, -3.7760e-03,  3.0486e-03, -8.8822e-03]],\n",
       " \n",
       "          [[-1.7966e-02,  1.3583e-02, -5.8766e-03,  3.4496e-02, -2.3546e-02],\n",
       "           [ 2.3712e-02, -3.3320e-02,  2.1296e-02,  1.5185e-02, -7.7071e-03],\n",
       "           [ 3.4259e-02, -2.5653e-02, -2.9366e-02, -2.4018e-02,  2.6309e-02],\n",
       "           [ 3.2582e-02,  1.8757e-03, -1.9622e-02, -3.3094e-02,  1.0224e-02],\n",
       "           [-7.6744e-03, -8.7646e-03, -1.2938e-02, -1.8594e-02, -3.2727e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.4466e-02, -1.8186e-02,  3.3685e-02,  2.5247e-02,  2.9399e-02],\n",
       "           [-1.8634e-02, -2.5211e-02,  1.4948e-02, -3.4504e-02, -1.8165e-02],\n",
       "           [-1.2844e-02,  2.5829e-02,  2.6774e-02, -3.4790e-04, -2.9982e-03],\n",
       "           [ 2.5854e-02,  5.9555e-03,  7.9400e-03,  1.7732e-02,  2.1812e-02],\n",
       "           [ 1.6321e-02, -1.2911e-02,  6.2451e-05, -1.8717e-02,  2.5955e-02]],\n",
       " \n",
       "          [[ 2.9877e-02,  4.2024e-03,  1.3534e-02, -8.4722e-03, -2.9951e-02],\n",
       "           [ 1.8441e-02, -1.9064e-02, -2.8084e-02, -2.8564e-02,  6.8884e-04],\n",
       "           [ 1.3201e-02, -2.4504e-02,  1.5925e-02, -2.1353e-02,  2.3607e-02],\n",
       "           [-8.8911e-03, -6.5284e-03, -1.1046e-02, -2.9138e-02, -1.0561e-02],\n",
       "           [-3.2389e-02,  3.3186e-02, -1.5720e-02,  1.2142e-03,  2.5680e-02]],\n",
       " \n",
       "          [[ 9.8855e-03,  2.1150e-02,  2.0875e-02, -2.9781e-02,  9.9835e-03],\n",
       "           [-1.5865e-02,  2.7056e-02, -1.3997e-03,  1.4220e-02, -5.6369e-03],\n",
       "           [-4.4068e-03, -1.5423e-02, -2.8591e-02, -2.9602e-02, -3.1743e-02],\n",
       "           [ 1.0664e-02, -1.5568e-02,  2.4377e-02, -2.9121e-02,  1.9716e-02],\n",
       "           [-2.0631e-02, -1.7054e-02, -2.5245e-03, -3.3975e-02, -2.0132e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.8764e-03, -3.4993e-02,  4.5317e-03, -2.7784e-02, -1.5007e-03],\n",
       "           [-3.0050e-02, -2.9759e-02, -9.1201e-04, -1.1120e-02, -1.1405e-02],\n",
       "           [-9.8179e-03, -3.5595e-03,  1.1712e-02,  1.0983e-02,  1.7418e-02],\n",
       "           [ 2.7245e-02, -2.7013e-02,  4.9177e-03, -1.4249e-02, -3.2657e-02],\n",
       "           [-9.0119e-03, -8.5319e-03,  2.4336e-02, -5.6187e-03, -1.2399e-02]],\n",
       " \n",
       "          [[ 3.2440e-02,  2.5267e-03, -2.1283e-02,  2.4648e-02, -9.8988e-03],\n",
       "           [-1.2209e-02, -2.7630e-02,  2.0164e-04,  3.5995e-03,  2.4018e-02],\n",
       "           [ 2.7763e-02,  1.7843e-02, -3.5212e-02, -8.9437e-03,  2.9891e-02],\n",
       "           [-3.4700e-02, -2.8908e-02,  2.2972e-02,  1.6333e-03, -2.3768e-02],\n",
       "           [ 5.2671e-03, -5.8611e-03, -3.2032e-02,  3.2598e-03,  9.1277e-03]],\n",
       " \n",
       "          [[-4.6620e-03,  3.1732e-02, -2.6252e-02, -8.7709e-03, -3.0228e-02],\n",
       "           [ 1.6585e-02,  2.9129e-02,  1.4182e-02,  1.2473e-02,  1.0326e-03],\n",
       "           [-3.9416e-03,  5.7929e-03,  1.6780e-02,  1.7578e-02,  2.8262e-02],\n",
       "           [-5.4475e-05,  1.9761e-02, -2.0212e-02, -8.4200e-03, -2.5946e-02],\n",
       "           [-1.6473e-02,  5.9996e-03,  4.3496e-03,  5.1845e-03, -1.6641e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.5228e-02,  2.7844e-03,  3.1231e-02, -1.1451e-02, -2.1759e-02],\n",
       "           [-2.7745e-02, -2.9177e-02,  2.4140e-02, -3.2345e-03, -2.3406e-02],\n",
       "           [-1.3846e-02,  2.3671e-02, -2.1986e-02, -1.0228e-02,  2.7073e-02],\n",
       "           [ 5.1457e-03, -1.4011e-05, -3.5054e-02,  3.3515e-03, -1.7545e-02],\n",
       "           [ 9.9458e-03, -2.6806e-02, -2.5863e-02, -2.5967e-02, -1.8255e-03]],\n",
       " \n",
       "          [[-1.3962e-02, -2.1024e-02, -4.2122e-03,  5.8734e-03, -2.7140e-02],\n",
       "           [-1.5918e-02,  1.0503e-02,  1.8463e-02, -4.0064e-03, -2.9892e-02],\n",
       "           [-2.2211e-03, -2.7838e-02,  6.9683e-03,  1.4696e-02, -2.5156e-03],\n",
       "           [ 1.2643e-02, -9.0398e-03, -1.6561e-02, -1.9064e-02,  1.0121e-02],\n",
       "           [ 3.0847e-02, -2.9288e-02,  2.6490e-02, -1.7661e-03,  3.4586e-02]],\n",
       " \n",
       "          [[-1.2304e-02, -2.9194e-02, -8.9285e-03, -1.1755e-02, -2.8509e-02],\n",
       "           [-3.0247e-02,  1.1671e-02,  3.3333e-02, -1.4599e-02, -9.2668e-03],\n",
       "           [-2.6599e-02,  1.5116e-02,  1.9983e-02,  7.6139e-03, -3.4504e-02],\n",
       "           [-1.0752e-02, -2.3226e-02, -3.2578e-02, -2.8427e-02, -2.3987e-03],\n",
       "           [ 2.5984e-02,  4.6090e-03,  1.8142e-02,  8.9494e-04,  2.8959e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.5968e-02,  1.4920e-02,  2.8568e-03,  3.3936e-02, -8.4337e-03],\n",
       "           [-3.4054e-02, -2.2409e-02, -6.5679e-03, -5.0403e-03, -3.4470e-02],\n",
       "           [ 1.7665e-03,  7.2973e-03, -1.0735e-03, -2.6310e-02, -1.4278e-02],\n",
       "           [-8.3866e-04,  1.9914e-02, -1.5668e-03,  1.4319e-02, -1.8326e-02],\n",
       "           [ 3.4337e-02,  2.2953e-03,  2.4151e-02, -1.9934e-02, -1.8595e-02]],\n",
       " \n",
       "          [[-4.7970e-03,  2.6497e-02, -2.0670e-02, -7.8221e-03, -2.2601e-02],\n",
       "           [-1.7328e-02,  3.4120e-02,  2.9174e-02, -1.3319e-03, -2.0406e-02],\n",
       "           [-2.6504e-02,  3.2130e-02,  3.3571e-02, -3.4758e-02, -1.1614e-02],\n",
       "           [-2.9363e-02, -7.2060e-03,  1.3801e-02, -3.1231e-02, -1.2863e-02],\n",
       "           [-2.7261e-03,  2.5460e-03,  1.4864e-02,  2.8170e-02, -6.0307e-03]],\n",
       " \n",
       "          [[ 2.6194e-02,  1.5018e-02, -3.5336e-02,  2.9685e-02,  2.0069e-02],\n",
       "           [ 2.3683e-02,  1.7394e-03, -1.2678e-02,  1.1823e-02, -1.2673e-02],\n",
       "           [ 2.0981e-02,  3.1580e-02,  1.9383e-02,  2.0926e-03,  2.7921e-03],\n",
       "           [-3.1330e-02,  2.0955e-02,  2.9931e-02, -6.5308e-03, -1.9580e-02],\n",
       "           [-2.6608e-02, -8.4868e-03, -2.5644e-02,  2.5781e-02, -8.8161e-03]]],\n",
       " \n",
       " \n",
       "         [[[-7.2252e-03, -2.1879e-03,  3.7199e-03,  3.4320e-02, -1.7821e-02],\n",
       "           [ 1.4449e-02,  2.8492e-02, -3.2309e-02,  1.3222e-02, -1.9710e-02],\n",
       "           [-8.3822e-03,  9.2256e-03, -2.4710e-02, -1.8101e-02, -1.3853e-02],\n",
       "           [-3.3883e-02, -4.7894e-03, -2.9559e-03, -1.6439e-02, -1.6131e-03],\n",
       "           [-1.6216e-02,  2.8280e-02,  1.8276e-02, -3.4060e-02, -3.4968e-02]],\n",
       " \n",
       "          [[-7.2134e-03,  2.9498e-02,  2.4353e-03,  1.7824e-02, -2.0978e-02],\n",
       "           [ 1.8021e-03,  1.5204e-02, -4.4168e-03, -2.7708e-02, -2.7714e-02],\n",
       "           [ 1.2126e-02,  3.0986e-02,  3.4987e-02,  8.4735e-03, -1.0829e-02],\n",
       "           [ 3.3717e-02, -1.6543e-02,  1.8921e-02, -2.4188e-02,  2.7651e-02],\n",
       "           [ 3.3029e-02, -9.5171e-03,  1.0869e-04, -1.9210e-02,  1.7752e-02]],\n",
       " \n",
       "          [[-3.2431e-02, -1.3786e-02, -1.6100e-02,  4.5421e-03,  2.7913e-03],\n",
       "           [-2.0976e-02, -1.3545e-02, -3.0273e-02, -2.9529e-02,  8.3753e-03],\n",
       "           [-2.1584e-02, -2.5854e-02,  1.3949e-02,  1.6684e-03,  3.5000e-02],\n",
       "           [-3.3822e-02,  3.2445e-02,  1.8656e-02,  1.2289e-02, -2.7966e-03],\n",
       "           [ 5.3690e-04,  2.6959e-03,  1.3396e-02, -1.5065e-02,  1.7881e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.4349e-02, -8.4216e-03, -1.9472e-02, -1.2981e-02,  2.6721e-02],\n",
       "           [-3.0342e-03,  1.1919e-02,  1.7886e-02,  1.7095e-02,  9.0000e-03],\n",
       "           [ 3.3095e-02,  1.0642e-02,  1.9105e-02,  2.9641e-02,  5.9674e-03],\n",
       "           [-8.6870e-03, -2.0585e-03, -1.7205e-02, -3.4049e-02,  6.6119e-04],\n",
       "           [-2.6279e-02, -1.4785e-02, -2.8855e-02,  2.8039e-02, -1.6393e-02]],\n",
       " \n",
       "          [[ 2.9751e-02,  1.2670e-02, -2.8308e-02,  1.8561e-02, -2.2095e-02],\n",
       "           [ 2.8570e-02,  2.9329e-02,  2.1976e-02, -2.0537e-02,  2.6738e-04],\n",
       "           [ 1.9055e-02,  1.2962e-02, -3.2293e-02, -3.1997e-02, -3.4585e-02],\n",
       "           [ 2.3999e-02,  7.5763e-03,  2.1476e-03, -5.8605e-03, -1.1020e-02],\n",
       "           [ 1.3971e-02, -2.6266e-02, -3.3110e-02,  3.3412e-03,  3.0473e-02]],\n",
       " \n",
       "          [[ 2.3184e-02, -3.0750e-02,  1.4938e-02,  1.8210e-02, -1.3479e-02],\n",
       "           [-2.9432e-03,  2.0004e-03, -2.5609e-02, -2.4036e-02,  5.9302e-03],\n",
       "           [ 1.4067e-03, -1.0444e-02,  3.5110e-02, -1.9563e-02,  6.2245e-03],\n",
       "           [-3.1574e-03, -6.7924e-03, -2.6930e-02, -2.4668e-02, -2.4099e-02],\n",
       "           [ 3.2745e-02, -2.1116e-03, -2.7856e-02,  1.4267e-03,  1.6211e-02]]]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0330, -0.0113, -0.0264,  0.0133,  0.0032, -0.0042,  0.0005,  0.0062,\n",
       "         -0.0017, -0.0145, -0.0017,  0.0182,  0.0174, -0.0228, -0.0251,  0.0235,\n",
       "          0.0024,  0.0181, -0.0233, -0.0044, -0.0278,  0.0281, -0.0115,  0.0303,\n",
       "         -0.0332, -0.0308, -0.0304,  0.0035, -0.0219, -0.0233, -0.0037,  0.0232],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0261,  0.0336,  0.0372,  ...,  0.0301,  0.0366,  0.0188],\n",
       "         [ 0.0246,  0.0167,  0.0018,  ...,  0.0005, -0.0075,  0.0198]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0281, -0.0258], requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(policy_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "^^^^^^^^^^^^^\n",
    "\n",
    "Finally, the code for training our model.\n",
    "\n",
    "Here, you can find an ``optimize_model`` function that performs a\n",
    "single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
    "loss. By defition we set $V(s) = 0$ if $s$ is a terminal\n",
    "state. We also use a target network to compute $V(s_{t+1})$ for\n",
    "added stability. The target network has its weights kept frozen most of\n",
    "the time, but is updated with the policy network's weights every so often.\n",
    "This is usually a set number of steps but we shall use episodes for\n",
    "simplicity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    import pdb; pdb.set_trace()\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and initialize the ``state`` Tensor. Then, we sample\n",
    "an action, execute it, observe the next screen and the reward (always\n",
    "1), and optimize our model once. When the episode ends (our model\n",
    "fails), we restart the loop.\n",
    "\n",
    "Below, `num_episodes` is set small. You should download\n",
    "the notebook and run lot more epsiodes, such as 300+ for meaningful\n",
    "duration improvements.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-19-832e6027db8a>(12)optimize_model()\n",
      "-> non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-19-832e6027db8a>(13)optimize_model()\n",
      "-> batch.next_state)), device=device, dtype=torch.bool)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-19-832e6027db8a>(14)optimize_model()\n",
      "-> non_final_next_states = torch.cat([s for s in batch.next_state\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-19-832e6027db8a>(16)optimize_model()\n",
      "-> state_batch = torch.cat(batch.state)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-19-832e6027db8a>(17)optimize_model()\n",
      "-> action_batch = torch.cat(batch.action)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  state_batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[-0.2865,  0.1154, -0.2236,  0.0395,  0.2479],\n",
       "           [-0.6749,  0.2786, -0.3888, -0.1706,  0.2271],\n",
       "           [-0.6876,  0.3215, -0.0703, -0.1527,  0.2203],\n",
       "           [-0.4525,  0.1850, -0.0962, -0.1410,  0.2795],\n",
       "           [-0.4272, -0.0053, -0.0641, -0.2672,  0.0624]],\n",
       " \n",
       "          [[-0.2297, -0.2393, -0.1607,  0.0609,  0.0705],\n",
       "           [-0.5900,  0.2567, -0.5175, -0.2145,  0.1034],\n",
       "           [-0.7560,  0.3395, -0.2837, -0.1607,  0.1404],\n",
       "           [-0.6215,  0.2732, -0.1656, -0.2681,  0.1703],\n",
       "           [-0.3327,  0.0355, -0.0819, -0.3951,  0.3081]],\n",
       " \n",
       "          [[-0.2599, -0.2337, -0.1467,  0.1062,  0.2307],\n",
       "           [-0.7620,  0.0616, -0.4532, -0.2038,  0.2526],\n",
       "           [-0.6099,  0.2066, -0.2482, -0.0602,  0.0849],\n",
       "           [-0.7150,  0.1406, -0.1796, -0.1892,  0.1153],\n",
       "           [-0.5386,  0.2043, -0.1390, -0.4058,  0.2083]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0675,  0.0239, -0.5066, -0.1031,  0.1656],\n",
       "           [-0.2009, -0.5979, -0.1844,  0.1022,  0.0782],\n",
       "           [-0.1822, -0.7349, -0.4336,  0.0466,  0.0920],\n",
       "           [-0.3709, -0.3927,  0.0302,  0.1239,  0.1376],\n",
       "           [-0.0471, -0.2406,  0.0119,  0.3393,  0.1299]],\n",
       " \n",
       "          [[-0.1116, -0.1373, -0.0957, -0.1276, -0.0947],\n",
       "           [-0.2038, -0.8791, -0.0425,  0.2335,  0.0082],\n",
       "           [-0.3739, -0.7488, -0.0053,  0.1737, -0.1003],\n",
       "           [-0.4988, -0.6019,  0.0759,  0.1105,  0.1939],\n",
       "           [-0.3866, -0.3205,  0.1045,  0.2844,  0.0040]],\n",
       " \n",
       "          [[-0.0843, -0.0802, -0.0808, -0.1347, -0.2094],\n",
       "           [-0.3674, -0.9049,  0.0519,  0.1509, -0.0557],\n",
       "           [-0.2188, -0.8333, -0.0497,  0.1249, -0.0934],\n",
       "           [-0.4685, -0.7219,  0.0609,  0.1492,  0.0101],\n",
       "           [-0.4383, -0.5821,  0.0950,  0.2088,  0.0846]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1411,  0.1422,  0.0462, -0.1989, -0.3986],\n",
       "           [ 0.2664,  0.0785,  0.1526, -0.3560, -0.1581],\n",
       "           [ 0.1401,  0.0368, -0.0528, -0.1973, -0.2762],\n",
       "           [-0.0237, -0.0470,  0.0352, -0.0498, -0.1847],\n",
       "           [-0.2498,  0.2434, -0.0029, -0.2278, -0.0499]],\n",
       " \n",
       "          [[ 0.1915,  0.2428,  0.0813, -0.1693, -0.4819],\n",
       "           [ 0.1447,  0.3282,  0.1405, -0.4259, -0.2279],\n",
       "           [ 0.3112,  0.2013,  0.1611, -0.2015, -0.1678],\n",
       "           [ 0.1089,  0.1349,  0.2800, -0.3938, -0.2744],\n",
       "           [ 0.3126,  0.2807, -0.0552, -0.1893, -0.2943]],\n",
       " \n",
       "          [[ 0.1920,  0.1611, -0.0493, -0.0309, -0.4752],\n",
       "           [ 0.2115,  0.1963,  0.1823, -0.4314, -0.2594],\n",
       "           [ 0.2186,  0.2560, -0.0201, -0.2360, -0.2614],\n",
       "           [ 0.1644,  0.1822,  0.1454, -0.4662, -0.2106],\n",
       "           [ 0.2591,  0.2844,  0.1357, -0.2701, -0.1311]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.1088, -0.1782, -0.1972, -0.1294,  0.0356],\n",
       "           [-0.2523, -0.4431, -0.0644,  0.1383, -0.0340],\n",
       "           [-0.1928, -0.2232, -0.1846,  0.2404, -0.2744],\n",
       "           [-0.3280, -0.3875, -0.1628,  0.2600, -0.1808],\n",
       "           [-0.3031, -0.2411,  0.0051,  0.3079, -0.2580]],\n",
       " \n",
       "          [[-0.1660, -0.0863, -0.0535, -0.1832,  0.0081],\n",
       "           [-0.2118, -0.4498, -0.1754,  0.0824, -0.1459],\n",
       "           [-0.3085, -0.4869, -0.0125,  0.1848, -0.4475],\n",
       "           [-0.3963, -0.3562, -0.1366,  0.2169, -0.5794],\n",
       "           [-0.2567, -0.4184,  0.1209,  0.2345, -0.4154]],\n",
       " \n",
       "          [[-0.2386, -0.1909, -0.0712, -0.1212,  0.0278],\n",
       "           [-0.4476, -0.4760,  0.0436, -0.0033, -0.2595],\n",
       "           [-0.4754, -0.5880, -0.0606,  0.0918, -0.3860],\n",
       "           [-0.4388, -0.4273, -0.0294,  0.1964, -0.4169],\n",
       "           [-0.2661, -0.3727, -0.0613,  0.2057, -0.2665]]],\n",
       " \n",
       " \n",
       "         [[[-0.2278, -0.0655,  0.2800, -0.1128, -0.1599],\n",
       "           [ 0.0105, -0.2008,  0.2707,  0.0286, -0.0996],\n",
       "           [-0.4507, -0.1946,  0.3371,  0.2047, -0.2045],\n",
       "           [-0.4918, -0.1326,  0.2067,  0.1862, -0.2948],\n",
       "           [-0.5094, -0.2354,  0.0548,  0.2412, -0.2850]],\n",
       " \n",
       "          [[-0.3003, -0.2441,  0.2108, -0.1163, -0.0408],\n",
       "           [-0.2958, -0.3230,  0.3748,  0.1282, -0.2561],\n",
       "           [-0.6306, -0.1281,  0.1856, -0.0139, -0.3721],\n",
       "           [-0.7105, -0.2467,  0.2545,  0.2186, -0.3390],\n",
       "           [-0.8404, -0.1116,  0.0362,  0.2597, -0.3300]],\n",
       " \n",
       "          [[-0.3910, -0.0464,  0.3506, -0.0981, -0.1697],\n",
       "           [-0.3588, -0.4698,  0.3173, -0.0092, -0.1439],\n",
       "           [-0.6345, -0.2777,  0.1495,  0.1967, -0.5687],\n",
       "           [-0.6269, -0.2803,  0.1754,  0.1701, -0.3939],\n",
       "           [-0.7157, -0.1293, -0.0393,  0.2113, -0.4125]]],\n",
       " \n",
       " \n",
       "         [[[-0.4421, -0.2089,  0.1094, -0.2004,  0.1503],\n",
       "           [-0.3650, -0.1002, -0.1226, -0.2256,  0.1074],\n",
       "           [-0.4169,  0.1705, -0.1717, -0.3107, -0.0089],\n",
       "           [-0.4518, -0.1217, -0.1698, -0.1586, -0.1362],\n",
       "           [-0.4262, -0.4244,  0.0128, -0.0343, -0.2752]],\n",
       " \n",
       "          [[-0.6222, -0.2902, -0.0042, -0.0428,  0.0596],\n",
       "           [-0.4437, -0.1497,  0.1395, -0.4018,  0.0504],\n",
       "           [-0.4345, -0.0065, -0.1485, -0.4608,  0.0261],\n",
       "           [-0.5885, -0.1851, -0.0830, -0.2972, -0.1448],\n",
       "           [-0.3936, -0.7555, -0.1273, -0.2360, -0.1640]],\n",
       " \n",
       "          [[-0.4918, -0.2426, -0.0332,  0.0315,  0.0375],\n",
       "           [-0.4712, -0.2791,  0.0046, -0.5185,  0.0888],\n",
       "           [-0.4980,  0.0470,  0.1465, -0.3349, -0.0119],\n",
       "           [-0.5231, -0.2117, -0.1381, -0.3323, -0.0196],\n",
       "           [-0.4168, -0.3785, -0.1330, -0.1760, -0.1519]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2008,  0.2784, -0.1554,  0.1088, -0.5758, -0.3120, -0.0876, -0.1256,\n",
       "          0.2030,  0.0766, -0.0220, -0.2390,  0.0338, -0.1775,  0.3941,  0.0618],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0268, 1.4013, 0.7585, 0.9298, 1.1375, 0.8041, 0.9243, 0.9326, 0.9922,\n",
       "         0.6799, 1.0772, 0.7626, 1.0256, 0.9985, 1.1157, 0.7186],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5231, -0.2308, -0.5583, -0.3414,  0.5551, -0.5104, -0.2482, -0.2803,\n",
       "         -0.1289, -0.4709,  0.4973, -0.3810,  0.7821, -0.2764, -0.2676, -0.3991],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 0.1127,  0.4798,  0.1766, -0.2503,  0.0561],\n",
       "           [ 0.1565,  0.5167, -0.1457, -0.1736, -0.0334],\n",
       "           [-0.3038,  0.0670, -0.1019, -0.0696, -0.1743],\n",
       "           [-0.5482, -0.2770, -0.2914, -0.0952, -0.3815],\n",
       "           [-0.3363, -0.3829, -0.1619, -0.0615, -0.1928]],\n",
       " \n",
       "          [[-0.1153,  0.1688,  0.0605,  0.1160, -0.1013],\n",
       "           [-0.0496, -0.0988, -0.0735,  0.0854, -0.1043],\n",
       "           [ 0.0853, -0.0372, -0.1798,  0.0010, -0.1747],\n",
       "           [ 0.0642, -0.2695, -0.0866,  0.2824, -0.1077],\n",
       "           [-0.0722, -0.2926, -0.0426,  0.3799, -0.0926]],\n",
       " \n",
       "          [[-0.1174, -0.1657, -0.0861, -0.0122, -0.0379],\n",
       "           [-0.4505, -0.1316, -0.1045, -0.1552, -0.2069],\n",
       "           [-0.3378, -0.1302, -0.1631,  0.1736, -0.1581],\n",
       "           [-0.2086, -0.0894, -0.1400,  0.2804, -0.3149],\n",
       "           [-0.4318, -0.0555, -0.2070,  0.3234, -0.2037]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.1916,  0.2135,  0.1469, -0.0973, -0.0469],\n",
       "           [-0.1164,  0.1506, -0.0820, -0.0814, -0.1008],\n",
       "           [-0.0130,  0.0534, -0.0773, -0.0474, -0.1466],\n",
       "           [ 0.1217, -0.1776, -0.1030, -0.1345, -0.0599],\n",
       "           [ 0.2695, -0.0593, -0.0516, -0.0502, -0.0929]],\n",
       " \n",
       "          [[-0.1918,  0.2829,  0.0741, -0.0640, -0.0657],\n",
       "           [-0.2280,  0.2897,  0.0979,  0.2010, -0.0912],\n",
       "           [-0.2821,  0.0488, -0.0811,  0.0030, -0.1401],\n",
       "           [-0.1357, -0.0260, -0.1421,  0.0926, -0.0560],\n",
       "           [ 0.1933, -0.1137, -0.1349, -0.0407, -0.1218]],\n",
       " \n",
       "          [[-0.0825,  0.1245,  0.0894, -0.0900,  0.0301],\n",
       "           [-0.3113,  0.0329, -0.1034, -0.0786, -0.1553],\n",
       "           [-0.1049, -0.0916, -0.1659,  0.2759, -0.0874],\n",
       "           [-0.3410, -0.1644, -0.1577, -0.0244, -0.0629],\n",
       "           [-0.2025, -0.2166, -0.2836, -0.1928, -0.1136]]],\n",
       " \n",
       " \n",
       "         [[[-0.3073,  0.5755,  0.0442,  0.3340,  0.0419],\n",
       "           [-0.1633,  0.7249, -0.0498,  0.3241, -0.0145],\n",
       "           [-0.2831,  0.4219, -0.1453, -0.0289,  0.1033],\n",
       "           [-0.1787,  0.2442, -0.1440, -0.0274, -0.0375],\n",
       "           [-0.1831, -0.0537, -0.1659,  0.1366,  0.2565]],\n",
       " \n",
       "          [[ 0.0854,  0.2323,  0.2088,  0.1094,  0.1871],\n",
       "           [ 0.3076,  0.1279,  0.1768,  0.1676,  0.4694],\n",
       "           [ 0.3090, -0.0156,  0.1570, -0.0023,  0.4472],\n",
       "           [ 0.3589, -0.0561, -0.0971, -0.0949,  0.3568],\n",
       "           [ 0.4596, -0.2181, -0.0783, -0.0751,  0.0482]],\n",
       " \n",
       "          [[ 0.5873,  0.3148, -0.0205,  0.3229,  0.4166],\n",
       "           [ 0.3099,  0.1724, -0.0527,  0.2977,  0.3900],\n",
       "           [-0.1332,  0.3011, -0.2164,  0.2445,  0.3305],\n",
       "           [-0.1669, -0.0466, -0.1527,  0.0827,  0.1707],\n",
       "           [-0.0783, -0.0961, -0.2223,  0.1175,  0.0026]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.1714,  0.3417,  0.3027, -0.0393,  0.1560],\n",
       "           [ 0.3313,  0.2110,  0.0552, -0.2476,  0.3831],\n",
       "           [ 0.4951, -0.0033,  0.0296, -0.0530,  0.3755],\n",
       "           [ 0.4201, -0.1163, -0.0225, -0.1563,  0.0920],\n",
       "           [ 0.2616, -0.3467, -0.0872, -0.0198,  0.0641]],\n",
       " \n",
       "          [[ 0.3304,  0.1975,  0.0060, -0.2825,  0.4938],\n",
       "           [ 0.4448,  0.2399,  0.1831, -0.3084,  0.2345],\n",
       "           [ 0.4670,  0.1061,  0.2277, -0.1376,  0.1850],\n",
       "           [ 0.2904, -0.0810,  0.1786, -0.1981,  0.1285],\n",
       "           [ 0.3667, -0.3610,  0.2160, -0.1420,  0.1394]],\n",
       " \n",
       "          [[-0.1629,  0.4975,  0.3619,  0.0900,  0.3957],\n",
       "           [-0.1285,  0.5122,  0.0905,  0.0816,  0.3004],\n",
       "           [-0.1270,  0.0299,  0.1350,  0.1013,  0.1002],\n",
       "           [ 0.0908, -0.1625,  0.1081,  0.0773,  0.1467],\n",
       "           [-0.0791, -0.1635,  0.2270,  0.0184,  0.1833]]],\n",
       " \n",
       " \n",
       "         [[[-0.3655,  0.4607,  0.2079,  0.4375,  0.0548],\n",
       "           [-0.3180,  0.5098,  0.2430,  0.4910,  0.1136],\n",
       "           [-0.1085,  0.3340,  0.3139,  0.4387,  0.1452],\n",
       "           [-0.2906,  0.1170,  0.1336,  0.2919,  0.2543],\n",
       "           [-0.2790, -0.0713,  0.1256,  0.3717,  0.1621]],\n",
       " \n",
       "          [[-0.1955, -0.0083,  0.0839,  0.4405, -0.0013],\n",
       "           [-0.1008,  0.0015,  0.2258,  0.5693,  0.0391],\n",
       "           [ 0.1217,  0.0278,  0.2376,  0.5411,  0.0588],\n",
       "           [ 0.2558, -0.0368,  0.1787,  0.4050,  0.0846],\n",
       "           [ 0.3154, -0.2483,  0.1949,  0.4476,  0.1521]],\n",
       " \n",
       "          [[ 0.2989,  0.5205,  0.0380, -0.0912,  0.2444],\n",
       "           [ 0.2641,  0.4402,  0.1560, -0.1025,  0.3024],\n",
       "           [ 0.3139,  0.4420,  0.3076,  0.0629,  0.2820],\n",
       "           [ 0.2379,  0.4421,  0.2276,  0.0830,  0.0756],\n",
       "           [ 0.0264,  0.0973,  0.1575,  0.1173,  0.0336]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.1944, -0.0141,  0.1923,  0.3483,  0.0935],\n",
       "           [-0.1762,  0.0113,  0.2734,  0.4450,  0.0613],\n",
       "           [ 0.1181, -0.0684,  0.2974,  0.3798,  0.0736],\n",
       "           [ 0.3220, -0.3153,  0.2482,  0.3011,  0.1482],\n",
       "           [ 0.3851, -0.3699,  0.1819,  0.3493,  0.1537]],\n",
       " \n",
       "          [[-0.0538,  0.1475,  0.2063,  0.2369,  0.1061],\n",
       "           [ 0.2133,  0.1277,  0.2770,  0.3239,  0.1546],\n",
       "           [ 0.3869, -0.0402,  0.2551,  0.2867,  0.1635],\n",
       "           [ 0.4114, -0.1114,  0.2357,  0.2354,  0.2068],\n",
       "           [ 0.2656, -0.2475,  0.2064,  0.3646,  0.2719]],\n",
       " \n",
       "          [[-0.1330,  0.0492,  0.2670,  0.4839,  0.0435],\n",
       "           [-0.0043,  0.0918,  0.3774,  0.4714,  0.1102],\n",
       "           [ 0.1525,  0.1365,  0.3998,  0.4799,  0.0851],\n",
       "           [ 0.0463,  0.0206,  0.3157,  0.2267,  0.1021],\n",
       "           [-0.0426, -0.1221,  0.2289,  0.3168,  0.1681]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.1614,  0.2498,  0.2975,  0.4083,  0.0069],\n",
       "           [-0.1258,  0.1897,  0.3648,  0.4420,  0.0548],\n",
       "           [-0.1706,  0.1491,  0.4338,  0.5341,  0.1326],\n",
       "           [-0.2486,  0.1089,  0.3771,  0.3338,  0.1548],\n",
       "           [-0.0666,  0.0724,  0.2144,  0.4674, -0.1739]],\n",
       " \n",
       "          [[ 0.1693,  0.1273,  0.2401,  0.3990,  0.0584],\n",
       "           [ 0.1666,  0.0347,  0.2065,  0.4134,  0.1324],\n",
       "           [ 0.1909, -0.2239,  0.1622,  0.4005,  0.1937],\n",
       "           [ 0.0871,  0.0237,  0.1555,  0.3247,  0.1351],\n",
       "           [ 0.0668, -0.2177,  0.1782,  0.4089,  0.0164]],\n",
       " \n",
       "          [[ 0.1289,  0.4608,  0.0172,  0.1194,  0.2761],\n",
       "           [ 0.2174,  0.3911,  0.1475,  0.1771,  0.2926],\n",
       "           [ 0.0609,  0.2551,  0.1191,  0.0892,  0.2832],\n",
       "           [-0.0015, -0.0623,  0.1010,  0.0796,  0.1770],\n",
       "           [-0.0593, -0.0683,  0.1201, -0.0100, -0.0833]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.1511,  0.1368,  0.2949,  0.3628,  0.0991],\n",
       "           [ 0.2221,  0.0822,  0.3113,  0.3965,  0.1747],\n",
       "           [ 0.2692, -0.1933,  0.2040,  0.3845,  0.2743],\n",
       "           [ 0.3467, -0.1171,  0.2372,  0.3590,  0.2145],\n",
       "           [ 0.0375, -0.1465,  0.1381,  0.3374,  0.0592]],\n",
       " \n",
       "          [[ 0.1589,  0.0702,  0.2140,  0.3282,  0.1822],\n",
       "           [ 0.2044,  0.0361,  0.2158,  0.3589,  0.2966],\n",
       "           [ 0.2646,  0.0876,  0.2075,  0.3362,  0.2603],\n",
       "           [ 0.2270, -0.0971,  0.2457,  0.1570,  0.3278],\n",
       "           [ 0.1558, -0.1795,  0.1730,  0.1910,  0.1649]],\n",
       " \n",
       "          [[ 0.0634,  0.1202,  0.3088,  0.3813,  0.1755],\n",
       "           [ 0.1308,  0.1213,  0.4047,  0.3288,  0.1578],\n",
       "           [ 0.0603, -0.1294,  0.3709,  0.4076,  0.1565],\n",
       "           [ 0.0512, -0.0969,  0.5153,  0.3296, -0.1771],\n",
       "           [ 0.0466,  0.1301,  0.3493,  0.2538, -0.1530]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0487,  0.1336, -0.2035, -0.3707,  0.2363],\n",
       "           [ 0.2300, -0.0287, -0.1642, -0.1773,  0.1950],\n",
       "           [ 0.1340, -0.0248, -0.0757, -0.0584, -0.2165],\n",
       "           [-0.1737, -0.3469, -0.0029, -0.1125, -0.1799],\n",
       "           [-0.2179, -0.4760, -0.1740, -0.3561, -0.0851]],\n",
       " \n",
       "          [[-0.1365,  0.0181, -0.5159, -0.0820,  0.3064],\n",
       "           [-0.0938, -0.0230, -0.4087,  0.1179,  0.1118],\n",
       "           [ 0.2894, -0.0617, -0.3134,  0.2331, -0.0768],\n",
       "           [ 0.3002, -0.4677,  0.1184,  0.0303,  0.2602],\n",
       "           [ 0.2052, -0.0276,  0.1592, -0.2893,  0.0793]],\n",
       " \n",
       "          [[-0.1078,  0.1617, -0.0951, -0.0641, -0.9343],\n",
       "           [-0.1384,  0.2105, -0.0827, -0.2619, -0.6752],\n",
       "           [-0.0555,  0.0186, -0.2311, -0.2723, -0.5908],\n",
       "           [-0.1017,  0.0155, -0.2610, -0.2874, -0.2986],\n",
       "           [ 0.0315, -0.1521, -0.2765, -0.3750, -0.2039]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.1449,  0.1186, -0.7372, -0.0796, -0.0292],\n",
       "           [ 0.2328,  0.0637, -0.3264,  0.1225,  0.0314],\n",
       "           [ 0.4129,  0.0029, -0.0376,  0.0045,  0.0134],\n",
       "           [ 0.4624, -0.0911,  0.2169, -0.1622, -0.0163],\n",
       "           [ 0.1849, -0.0704,  0.2638, -0.1253, -0.1416]],\n",
       " \n",
       "          [[ 0.0300,  0.2429, -0.4029, -0.3076, -0.0245],\n",
       "           [ 0.1904,  0.2644, -0.0242, -0.1636, -0.1007],\n",
       "           [ 0.3099,  0.0728,  0.0722, -0.0816, -0.0735],\n",
       "           [ 0.1723, -0.1491,  0.2455, -0.1434, -0.0558],\n",
       "           [ 0.1137, -0.3791,  0.3423, -0.1313, -0.3386]],\n",
       " \n",
       "          [[ 0.0492,  0.0138, -0.3470, -0.3018,  0.1602],\n",
       "           [ 0.1714,  0.1466, -0.2932, -0.3798,  0.0535],\n",
       "           [ 0.1139, -0.0192, -0.2054, -0.1318,  0.0362],\n",
       "           [ 0.0104, -0.2282, -0.1622, -0.1215, -0.2367],\n",
       "           [-0.0631, -0.2207, -0.1018, -0.0941, -0.0612]]],\n",
       " \n",
       " \n",
       "         [[[-0.3666,  0.0850,  0.1839,  0.4590, -0.0101],\n",
       "           [-0.3424,  0.1356,  0.1920,  0.3586,  0.0529],\n",
       "           [-0.1526,  0.1466,  0.3030,  0.4110,  0.0235],\n",
       "           [-0.1313,  0.0056,  0.1209,  0.2995,  0.1239],\n",
       "           [-0.0698, -0.0667,  0.1237,  0.3796,  0.1355]],\n",
       " \n",
       "          [[-0.0185,  0.1357,  0.1368,  0.5080,  0.0778],\n",
       "           [-0.0234,  0.0731,  0.1358,  0.4545,  0.1581],\n",
       "           [ 0.0965, -0.0087,  0.2322,  0.4467,  0.1773],\n",
       "           [ 0.1617, -0.0791,  0.2544,  0.3152,  0.1685],\n",
       "           [-0.0924, -0.1531,  0.2231,  0.3383,  0.2806]],\n",
       " \n",
       "          [[ 0.2333,  0.3792,  0.0958,  0.2554,  0.0871],\n",
       "           [ 0.2044,  0.3745,  0.0798,  0.2022,  0.2768],\n",
       "           [ 0.1623,  0.3860,  0.1850,  0.1687, -0.0015],\n",
       "           [ 0.1589,  0.2469,  0.1321,  0.1707, -0.1844],\n",
       "           [ 0.1863,  0.1438,  0.0853,  0.1410, -0.0933]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0433,  0.0461,  0.0800,  0.3706,  0.0281],\n",
       "           [ 0.0500, -0.0071,  0.1506,  0.4063,  0.0567],\n",
       "           [ 0.1323, -0.0761,  0.1940,  0.4121,  0.0946],\n",
       "           [ 0.2540, -0.2042,  0.2237,  0.2886,  0.1410],\n",
       "           [ 0.2334, -0.7281,  0.1807,  0.3255,  0.1313]],\n",
       " \n",
       "          [[ 0.1086,  0.0678,  0.0968,  0.2914,  0.0572],\n",
       "           [ 0.2548,  0.0918,  0.1705,  0.3069,  0.1578],\n",
       "           [ 0.2695, -0.0410,  0.2810,  0.3250,  0.2335],\n",
       "           [ 0.1899, -0.2485,  0.2965,  0.2364,  0.2237],\n",
       "           [ 0.2893, -0.0125,  0.4863,  0.2491,  0.1954]],\n",
       " \n",
       "          [[ 0.0344,  0.0778,  0.2995,  0.6393,  0.0719],\n",
       "           [ 0.0772,  0.0886,  0.3563,  0.5175,  0.1280],\n",
       "           [ 0.1130,  0.0423,  0.2399,  0.5365,  0.1328],\n",
       "           [ 0.0951, -0.0621,  0.2459,  0.2776,  0.0389],\n",
       "           [ 0.0554, -0.1420,  0.2691,  0.4247,  0.1119]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0150,  0.3336,  0.1195,  0.2550,  0.0678, -0.0574, -0.0376, -0.0238,\n",
       "          0.0880, -0.1224,  0.1556,  0.0265,  0.0977, -0.0521,  0.1641, -0.0388,\n",
       "         -0.0322,  0.0676,  0.1615, -0.0157,  0.1467, -0.1130,  0.0806,  0.0815,\n",
       "         -0.0843, -0.0615,  0.0656, -0.1320,  0.0063, -0.0306, -0.1165,  0.0841],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9307, 0.7700, 1.0475, 0.6484, 0.7422, 0.5346, 1.3121, 0.8501, 1.0280,\n",
       "         0.8368, 1.0556, 1.3790, 0.8063, 0.9305, 1.5277, 0.6113, 1.3378, 0.9010,\n",
       "         1.1758, 1.4046, 1.0785, 0.6855, 0.7086, 1.2019, 0.9266, 1.3559, 1.1382,\n",
       "         1.0094, 0.8763, 1.1086, 1.3684, 1.0427], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0155, -0.8138, -0.3301, -0.0514, -0.5970, -0.1012,  0.2580,  0.3430,\n",
       "          0.2929, -0.0112,  0.0626, -0.2017, -0.3430, -0.1439,  0.0775, -0.0025,\n",
       "          0.0633,  0.4052, -0.1456, -0.3897, -0.4660, -0.2698, -0.6030, -0.1235,\n",
       "          0.4805,  0.5243,  0.8149,  0.4322, -0.1646, -0.5448,  0.7077, -0.4162],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-1.7658e-01, -8.0806e-02, -2.8707e-01, -1.1867e-01,  4.0537e-01],\n",
       "           [ 8.2998e-02, -3.7421e-02, -6.2796e-02, -1.8237e-01,  7.2766e-02],\n",
       "           [ 6.8467e-02, -1.3624e-01, -2.0201e-01, -1.0705e-02, -1.9307e-01],\n",
       "           [ 7.9511e-02,  1.0816e-02, -2.1265e-01,  2.1649e-01,  4.5535e-02],\n",
       "           [ 1.1003e-01,  1.7217e-02, -2.6742e-01, -6.1002e-01,  2.8840e-02]],\n",
       " \n",
       "          [[-2.5694e-01, -2.3350e-01, -1.8845e-01, -3.6067e-01,  2.1213e-01],\n",
       "           [-3.4380e-01, -3.1020e-01,  3.9987e-02, -2.2973e-01, -1.0372e-01],\n",
       "           [-1.5753e-01, -5.3389e-01, -2.0590e-03, -1.4723e-01,  3.7037e-02],\n",
       "           [-4.1610e-02, -2.6678e-01, -2.3735e-01, -6.5538e-01,  6.5524e-02],\n",
       "           [ 1.1187e-01, -3.4292e-01, -1.1779e-01, -1.9402e-01,  3.8544e-02]],\n",
       " \n",
       "          [[-9.1386e-02, -4.6944e-02, -3.8207e-01,  8.4436e-02, -1.8893e-02],\n",
       "           [-1.7230e-01, -2.6518e-01, -3.6328e-01, -2.1287e-01, -2.5871e-01],\n",
       "           [-1.2305e-01, -2.9861e-01, -3.7000e-01, -2.6533e-01, -2.3118e-01],\n",
       "           [-6.7698e-02, -1.1736e-01, -4.6037e-01, -3.1288e-01, -1.6724e-01],\n",
       "           [ 9.3914e-02, -3.1431e-01, -2.0712e-01, -3.0328e-01, -1.2271e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.6327e-01, -3.0699e-01, -1.6265e-01, -1.0583e-02, -3.6584e-01],\n",
       "           [-3.9182e-01, -4.8183e-01, -1.2164e-01, -5.0196e-02, -7.4971e-02],\n",
       "           [-3.7303e-01, -3.2942e-01, -3.7535e-01, -8.5316e-02, -1.3117e-01],\n",
       "           [-4.9853e-02, -4.5190e-01, -4.4441e-01, -3.7304e-01,  3.2881e-03],\n",
       "           [-1.4207e-01, -6.1108e-02, -2.0982e-01, -6.9112e-01, -8.8159e-02]],\n",
       " \n",
       "          [[ 4.3482e-05,  2.5057e-01, -6.6708e-01, -3.1858e-01,  2.7191e-01],\n",
       "           [-1.5191e-01,  3.6974e-02, -2.2009e-01, -1.9520e-01,  3.0826e-01],\n",
       "           [-6.2504e-02, -5.1686e-02,  9.8806e-02, -8.3839e-02,  1.0855e-01],\n",
       "           [ 1.1222e-01,  2.3374e-02,  7.7102e-03,  2.1269e-02, -2.2335e-02],\n",
       "           [ 2.2906e-01, -1.3464e-01, -3.0036e-01, -4.7946e-01, -2.4523e-01]],\n",
       " \n",
       "          [[-7.7265e-02, -2.6918e-01, -1.2781e-01,  5.5396e-03, -5.6918e-02],\n",
       "           [-8.9454e-02, -2.8560e-01, -2.4634e-01, -2.0092e-02, -1.0710e-01],\n",
       "           [-4.5002e-02, -2.3460e-01, -3.5905e-01, -6.6577e-02, -8.5851e-02],\n",
       "           [-1.1183e-01, -2.0668e-01, -4.3234e-01, -1.4081e-01, -9.0804e-02],\n",
       "           [-5.7011e-04, -1.3105e-01, -2.6299e-01, -3.2428e-01, -2.4777e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.7752e-01,  4.8972e-02,  3.0417e-01,  1.0715e-01,  4.3359e-02],\n",
       "           [ 2.6125e-01,  5.9265e-02,  4.7781e-02,  6.6804e-02,  1.0576e-01],\n",
       "           [ 2.3313e-02,  1.9578e-01,  3.5647e-02,  1.0412e-01,  9.8399e-02],\n",
       "           [ 1.0644e-01,  2.5229e-01,  4.5778e-01,  1.0619e-01,  1.5377e-01],\n",
       "           [ 2.8119e-01, -8.4313e-02, -2.6957e-01,  4.8935e-02,  3.0378e-02]],\n",
       " \n",
       "          [[ 4.4795e-01,  3.3741e-01,  2.5616e-01,  1.5822e-01,  1.9885e-01],\n",
       "           [ 5.3818e-01,  3.7555e-01,  5.3733e-01,  1.1232e-01,  4.7066e-02],\n",
       "           [ 6.6531e-02,  7.2561e-02,  1.1555e-02, -1.0929e-01,  1.1153e-02],\n",
       "           [ 2.3164e-03,  1.5341e-01, -8.8347e-02, -3.5582e-01,  1.4140e-01],\n",
       "           [-3.3886e-02,  2.8877e-01,  5.6891e-02,  1.5743e-01,  1.7879e-02]],\n",
       " \n",
       "          [[ 9.5924e-02,  1.5985e-01,  3.7797e-01,  3.5309e-01,  2.9040e-01],\n",
       "           [-2.4300e-01,  1.9287e-01, -2.5146e-01,  2.8491e-01, -6.2766e-01],\n",
       "           [-2.6821e-01,  1.6844e-02, -2.4176e-01, -1.3050e-01, -5.7302e-01],\n",
       "           [-4.4657e-01, -5.8893e-02, -3.5796e-01, -3.7518e-01, -1.7487e-01],\n",
       "           [-9.2378e-02,  1.7581e-01, -7.8757e-01, -4.1593e-02, -6.8316e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.7797e-01,  2.8747e-01,  2.5300e-01,  1.5165e-01,  1.5762e-01],\n",
       "           [ 2.4549e-01,  2.7328e-01,  2.5360e-01,  5.1938e-01,  1.1677e-01],\n",
       "           [ 6.9927e-02,  5.2619e-02, -1.1287e-02, -2.9497e-01, -2.1090e-01],\n",
       "           [-3.3050e-02, -6.4545e-02, -2.1195e-01, -7.1687e-01, -1.1743e-04],\n",
       "           [-5.6133e-02,  3.0118e-01, -4.2174e-01,  8.3209e-02, -4.2178e-01]],\n",
       " \n",
       "          [[ 5.9953e-01,  3.0253e-01,  2.3338e-01,  5.8687e-02,  2.2502e-01],\n",
       "           [ 5.5456e-01,  3.1833e-01,  7.2073e-02,  7.0979e-02,  1.6680e-01],\n",
       "           [-5.5884e-02,  5.9238e-01,  8.5165e-02,  5.5799e-02,  1.1345e-02],\n",
       "           [ 2.3099e-01,  4.9755e-01,  1.0548e-01,  5.6006e-02,  1.0518e-01],\n",
       "           [ 4.4140e-01,  2.5151e-02, -4.8352e-02,  1.8113e-01, -8.2471e-02]],\n",
       " \n",
       "          [[ 2.8252e-01,  2.4302e-01,  2.1657e-01,  2.2819e-01,  2.0954e-01],\n",
       "           [ 5.4872e-02,  1.6006e-01,  2.3790e-02,  2.6817e-01,  2.4336e-02],\n",
       "           [-2.0598e-01,  6.4240e-02, -2.2201e-02, -2.1831e-01, -4.4934e-01],\n",
       "           [-2.1558e-01, -3.3865e-01, -3.2714e-03, -1.7154e-01, -1.9038e-01],\n",
       "           [-4.4188e-01,  1.1993e-01, -8.9055e-02,  1.2072e-01, -2.8066e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 8.4752e-02, -9.0271e-02, -7.5855e-02, -5.7558e-02,  9.9512e-02],\n",
       "           [ 1.0341e-01, -8.1855e-02, -1.1681e-01,  1.4309e-01,  9.9026e-02],\n",
       "           [ 1.1926e-01, -1.0750e-01, -8.3621e-02,  1.6932e-01,  1.1833e-01],\n",
       "           [ 1.3331e-01, -7.7687e-02, -1.1005e-01, -5.1875e-02,  6.6107e-02],\n",
       "           [ 1.1431e-01, -9.1831e-02, -7.9895e-02, -8.0383e-02, -8.1131e-02]],\n",
       " \n",
       "          [[-7.9518e-03, -1.7461e-01, -1.0575e-01, -9.1403e-02,  6.7627e-02],\n",
       "           [ 4.8023e-03,  9.7748e-03, -9.5102e-02, -1.1171e-01,  9.5362e-02],\n",
       "           [ 4.0599e-02,  3.6528e-02, -1.1841e-01, -8.0625e-02,  5.9483e-02],\n",
       "           [ 7.4767e-02,  3.1670e-02, -1.5661e-01, -8.8785e-02,  6.4404e-02],\n",
       "           [ 5.4885e-02, -1.4857e-01, -1.2985e-01,  7.6130e-02, -1.0541e-01]],\n",
       " \n",
       "          [[-2.2064e-01, -1.8064e-01, -1.5607e-01, -1.1531e-01, -1.0441e-01],\n",
       "           [-2.4812e-01, -2.6354e-01,  7.9654e-02, -1.0270e-01, -7.0191e-02],\n",
       "           [ 3.6877e-03, -1.2219e-01, -2.8975e-02, -1.3165e-01, -1.2045e-01],\n",
       "           [-1.7107e-01, -6.3856e-02,  6.1877e-02, -7.5159e-02, -1.2594e-01],\n",
       "           [ 2.8189e-02,  8.0056e-02, -1.1697e-01, -9.3209e-02,  1.1459e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.8345e-02, -1.0281e-01, -1.3792e-01, -9.2038e-02,  7.5239e-02],\n",
       "           [ 6.9877e-02, -1.0891e-01, -1.8560e-01, -9.6522e-02,  1.3392e-01],\n",
       "           [ 1.9907e-02, -3.3596e-01, -1.7136e-01, -1.1445e-01, -8.0154e-02],\n",
       "           [ 6.9658e-02, -2.8781e-02, -1.3865e-01, -1.0063e-01, -5.8008e-02],\n",
       "           [ 5.3629e-02, -1.9634e-01,  3.8983e-02, -7.2169e-03, -1.1518e-01]],\n",
       " \n",
       "          [[ 5.8954e-02, -5.4429e-02, -1.2042e-01,  1.4118e-01, -1.5035e-01],\n",
       "           [ 1.5127e-01, -7.4083e-02, -1.1497e-01, -8.7941e-02,  3.7258e-03],\n",
       "           [-2.4926e-02, -9.2841e-02, -7.5992e-02, -7.0670e-02,  1.1685e-01],\n",
       "           [-5.4210e-02,  1.3102e-02,  1.4410e-01, -8.4446e-02, -1.3219e-01],\n",
       "           [ 5.3378e-02,  1.4433e-01, -1.0128e-01, -1.0323e-01,  6.7649e-02]],\n",
       " \n",
       "          [[-1.4984e-01, -1.8516e-01, -1.8790e-01, -9.4310e-02, -1.2811e-01],\n",
       "           [-1.0560e-01, -2.0297e-01,  1.2412e-03,  8.8580e-02, -1.1367e-01],\n",
       "           [-8.7314e-02, -1.8945e-01, -3.6180e-02,  5.1995e-02, -7.8654e-02],\n",
       "           [-7.6780e-02, -1.7532e-01,  4.9135e-02,  5.8565e-02, -8.8054e-02],\n",
       "           [-1.2547e-01, -1.4727e-01,  6.4091e-02, -5.1465e-02, -1.2433e-01]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.3158e-01, -8.0912e-02, -2.3673e-01, -1.2845e-01, -1.3022e-01],\n",
       "           [-9.9062e-02, -5.7881e-02, -6.4634e-02, -1.8453e-01, -1.6557e-01],\n",
       "           [-1.8003e-01, -2.5394e-02, -2.6868e-02, -6.4801e-02, -2.0521e-01],\n",
       "           [-2.6375e-01, -1.1972e-01, -9.8992e-02, -6.2840e-02, -4.6353e-02],\n",
       "           [-5.9188e-03, -1.8116e-01, -2.2470e-02, -1.1458e-01, -2.9814e-02]],\n",
       " \n",
       "          [[-2.9640e-01,  1.4467e-02, -4.2614e-01, -3.7900e-01, -2.1224e-01],\n",
       "           [-1.8068e-01, -2.6307e-01, -5.7756e-01, -4.3989e-01, -2.6320e-01],\n",
       "           [-9.0078e-02, -2.9782e-01, -4.1117e-01, -2.9978e-01, -1.6903e-01],\n",
       "           [-2.1608e-01, -3.1593e-01, -5.3860e-01, -3.5935e-01, -1.5799e-01],\n",
       "           [-1.1472e-02, -2.2040e-01,  1.0408e-01,  4.7931e-01, -1.2250e-01]],\n",
       " \n",
       "          [[ 2.7999e-01, -1.8410e-01, -7.6866e-01, -7.9586e-02,  1.2652e-01],\n",
       "           [ 5.1559e-01, -1.3415e-01, -5.0553e-01, -1.3814e-01, -8.0171e-02],\n",
       "           [ 2.7565e-01,  9.2155e-02, -5.0413e-01, -4.2204e-01, -1.8250e-01],\n",
       "           [ 1.8895e-01, -1.9860e-02, -4.4775e-01, -6.4775e-01,  1.3806e-01],\n",
       "           [ 4.5555e-01, -1.4823e-01,  1.1741e-01, -4.1320e-01,  1.0320e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.8405e-02,  2.5276e-02, -7.0470e-01, -2.8720e-01,  2.3077e-01],\n",
       "           [-1.1266e-02, -1.1971e-01, -6.4849e-01, -2.3039e-01, -1.1252e-01],\n",
       "           [ 1.5628e-01, -1.9049e-01, -2.9408e-01, -3.1178e-01, -1.6059e-01],\n",
       "           [ 1.9172e-01,  4.9330e-03, -3.0606e-01, -4.9162e-01, -4.0001e-02],\n",
       "           [ 2.5090e-01,  4.9804e-01, -1.9758e-01, -1.1164e+00,  4.2132e-01]],\n",
       " \n",
       "          [[-6.8953e-01,  1.3599e-01,  3.3772e-01, -2.6933e-01, -3.7238e-01],\n",
       "           [-5.1924e-01,  1.7840e-01, -4.6575e-02, -2.5351e-01, -3.7624e-01],\n",
       "           [-4.2650e-01,  8.1798e-02, -9.5651e-02, -2.4762e-01, -4.5951e-01],\n",
       "           [-6.4154e-01, -6.8432e-02, -2.6998e-02, -6.2679e-02, -4.2969e-01],\n",
       "           [-3.2852e-01, -1.6240e-01,  6.2439e-02,  1.4544e-01, -1.1460e-01]],\n",
       " \n",
       "          [[ 4.5533e-02, -1.9358e-01, -8.0888e-01, -4.3751e-02, -1.5915e-01],\n",
       "           [ 2.3369e-01, -1.3821e-01, -7.4040e-01, -9.4941e-02, -1.8035e-01],\n",
       "           [ 4.1321e-01, -1.1589e-01, -5.7611e-01, -1.9903e-01, -1.3615e-01],\n",
       "           [ 2.3575e-01,  9.4662e-02, -5.5096e-01, -3.7073e-01, -9.8206e-02],\n",
       "           [ 2.3825e-01,  1.9035e-01, -2.1911e-01, -6.9791e-01,  8.1915e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.1283e-01, -8.6263e-02,  1.3360e-01, -1.0544e-01, -1.1589e-01],\n",
       "           [-1.2549e-01, -1.1436e-01,  1.2794e-01, -1.0219e-01, -1.1964e-01],\n",
       "           [-1.1340e-01,  1.7337e-01,  8.2015e-02,  9.1391e-02, -6.9239e-02],\n",
       "           [-9.6297e-02,  1.1369e-01,  7.0910e-02,  1.1080e-01, -1.0063e-01],\n",
       "           [-8.9989e-02, -1.2374e-01, -7.2856e-02,  9.1111e-02,  1.2075e-01]],\n",
       " \n",
       "          [[-1.3893e-01, -2.2818e-01,  1.7283e-02, -1.8681e-01, -1.8168e-01],\n",
       "           [-1.7984e-01, -2.0225e-01,  3.3725e-02, -1.5313e-03, -1.8808e-01],\n",
       "           [-1.7160e-01, -2.3315e-01,  2.0219e-02,  1.7013e-02, -1.6136e-01],\n",
       "           [-1.4022e-01, -1.9576e-01, -1.2191e-02, -5.3506e-03, -1.7366e-01],\n",
       "           [-1.3119e-01, -2.1714e-01,  5.2740e-02,  1.9877e-02, -1.5385e-01]],\n",
       " \n",
       "          [[-2.2263e-01, -2.0253e-01, -2.1516e-01,  9.4536e-03, -2.0033e-01],\n",
       "           [-4.2640e-02, -1.8061e-01, -1.7586e-01,  6.1320e-03,  1.3079e-02],\n",
       "           [-2.3630e-01, -1.8743e-01, -1.8886e-01,  1.7811e-02, -1.4223e-02],\n",
       "           [-2.1318e-01, -2.0000e-01, -2.3368e-01, -1.3788e-02,  1.7057e-02],\n",
       "           [-1.3834e-01, -1.2451e-01,  3.8536e-02,  5.0279e-02, -1.7404e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.3863e-01, -1.9125e-01, -1.8082e-03,  4.8052e-02, -1.7674e-01],\n",
       "           [-2.0284e-01, -2.3264e-01, -1.6410e-02,  2.5385e-03, -9.4643e-03],\n",
       "           [-1.5486e-01, -2.0187e-01, -2.1084e-01, -1.8593e-02,  7.8223e-03],\n",
       "           [-1.6846e-01, -1.7681e-01, -2.0884e-01,  1.9707e-02, -2.0187e-01],\n",
       "           [-9.1429e-02, -2.0675e-01, -1.2852e-01, -4.5577e-03,  1.1832e-02]],\n",
       " \n",
       "          [[-1.1528e-01, -1.5339e-01, -3.4704e-02, -1.5462e-02, -1.7368e-01],\n",
       "           [-1.3754e-01, -1.4255e-01,  5.4845e-02,  1.0372e-02, -2.1372e-01],\n",
       "           [-1.5821e-01, -1.0501e-01,  6.9834e-02, -4.4074e-02, -1.8117e-01],\n",
       "           [-1.5000e-01, -1.3108e-01, -8.5323e-03, -1.8628e-02, -1.9330e-01],\n",
       "           [-1.0741e-01, -1.1685e-01, -1.9021e-01,  9.7613e-02,  7.3191e-03]],\n",
       " \n",
       "          [[-1.5323e-01, -1.9239e-01, -2.3761e-01,  4.3071e-02, -1.5221e-01],\n",
       "           [ 1.9322e-02, -2.1284e-01, -2.0887e-01,  2.3996e-02,  1.2294e-02],\n",
       "           [ 3.6751e-02,  1.4127e-02, -1.8160e-01,  1.1257e-02,  3.3033e-02],\n",
       "           [-1.8770e-03, -1.9833e-01, -1.6570e-01, -2.0231e-01,  1.0143e-02],\n",
       "           [-1.7713e-01, -3.0579e-03, -1.9429e-01, -1.6197e-01,  2.1503e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.9191e-01, -1.5237e-02, -7.5984e-02,  2.5953e-02, -1.6023e-01],\n",
       "           [-8.6948e-02,  3.5855e-02, -1.3809e-01, -3.4836e-02, -2.3431e-01],\n",
       "           [-2.4924e-01,  2.0338e-02, -1.5060e-01, -6.5914e-02, -3.0863e-01],\n",
       "           [-3.3387e-01, -9.9724e-02, -7.9454e-02, -8.4410e-02, -9.0543e-02],\n",
       "           [ 9.7459e-02, -1.0285e-01, -6.4095e-02, -1.0815e-01, -1.1518e-01]],\n",
       " \n",
       "          [[-4.7607e-01, -6.7897e-02, -1.1693e-01,  3.0505e-02, -1.6833e-01],\n",
       "           [-4.7456e-01, -7.5876e-02, -1.4467e-01, -7.8255e-02, -5.6856e-01],\n",
       "           [-3.3785e-01, -2.5073e-02, -1.3266e-01, -5.9454e-02, -4.8405e-01],\n",
       "           [-4.7186e-01,  1.0332e-01, -2.2982e-01, -1.6741e-01, -1.9683e-01],\n",
       "           [ 4.6330e-02,  5.0408e-02, -6.8229e-02, -1.1333e-01, -1.7816e-01]],\n",
       " \n",
       "          [[-7.6800e-02, -3.6745e-01, -8.3946e-01, -1.5249e-01,  2.6430e-02],\n",
       "           [ 4.8069e-01, -4.0882e-01, -4.7146e-01, -1.4887e-01,  9.9608e-02],\n",
       "           [ 4.5298e-01, -4.3664e-01, -1.2727e-01, -2.4583e-01,  6.7094e-02],\n",
       "           [ 2.8495e-01, -5.6108e-01, -3.1120e-01, -3.3930e-01, -1.6611e-01],\n",
       "           [ 8.3520e-01, -3.2566e-01, -1.1405e-01, -2.9176e-01, -2.9916e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.5220e-01, -3.4814e-01, -4.2805e-01, -1.9575e-01,  6.0118e-02],\n",
       "           [-4.4585e-02, -4.6093e-01, -2.2937e-01, -4.7888e-02, -5.7784e-02],\n",
       "           [ 5.7252e-01, -4.8358e-01, -2.3815e-01, -9.6210e-02, -1.9331e-01],\n",
       "           [ 1.2676e-01, -5.8584e-01, -4.1132e-01, -3.3073e-01, -2.0834e-01],\n",
       "           [ 6.2287e-01, -1.9779e-01, -2.1562e-01, -4.1255e-01, -3.3155e-01]],\n",
       " \n",
       "          [[-3.5909e-01,  6.4840e-02, -1.3911e-01,  5.1456e-02, -2.3566e-01],\n",
       "           [-4.5540e-01,  1.2644e-01, -6.0609e-02,  3.4241e-02, -2.2309e-01],\n",
       "           [-3.9073e-01, -4.5510e-03, -6.9422e-02, -9.2197e-02, -3.3806e-01],\n",
       "           [-5.5390e-01, -5.8228e-02,  9.9546e-03, -7.9630e-02, -3.7765e-01],\n",
       "           [-2.0392e-01, -1.5206e-01,  5.3416e-02,  6.9387e-02, -2.1724e-01]],\n",
       " \n",
       "          [[ 8.6783e-03, -3.5789e-01, -3.4071e-01, -1.4725e-01, -7.7586e-02],\n",
       "           [ 8.0626e-02, -3.6441e-01, -3.2293e-01, -6.7070e-02, -1.6384e-01],\n",
       "           [ 1.1332e-01, -3.8584e-01, -2.4434e-01, -1.4145e-01, -2.3198e-01],\n",
       "           [-2.5838e-02, -4.1968e-01, -3.6028e-01, -2.9391e-01, -2.8046e-01],\n",
       "           [ 9.4336e-02, -1.9179e-01, -2.1688e-01, -3.7627e-01, -2.7053e-01]]]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0020, -0.0144, -0.0069,  0.0449, -0.0306, -0.0075,  0.0120,  0.0206,\n",
       "         -0.0318, -0.0320,  0.0092,  0.0129,  0.0238, -0.0246, -0.0019,  0.0355,\n",
       "          0.0022, -0.0150, -0.0436,  0.0363, -0.0030,  0.0226, -0.0241,  0.0155,\n",
       "         -0.0267,  0.0044, -0.0557,  0.0116, -0.0262, -0.0221, -0.0044,  0.0162],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.8700, 0.2771, 0.6579, 0.5377, 0.7623, 0.4838, 0.6815, 0.3026, 0.9222,\n",
       "         0.6497, 0.6890, 0.5342, 1.0182, 0.6876, 0.6866, 0.4565, 0.7170, 0.6075,\n",
       "         0.7323, 0.7043, 0.7882, 0.8325, 0.5273, 0.9019, 0.5740, 0.5347, 0.7399,\n",
       "         0.6957, 0.6851, 0.8258, 0.6608, 0.9460], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.4113, -0.5838, -0.3091, -0.3168, -0.3692, -0.2021, -0.3188, -0.5721,\n",
       "          0.1420, -0.3674, -0.2106, -0.1654,  0.2506, -0.3025, -0.2409, -0.6166,\n",
       "         -0.2741, -0.2738, -0.3307, -0.3287, -0.3480, -0.3405, -0.3690,  0.1761,\n",
       "         -0.3749, -0.2766, -0.3481, -0.1516, -0.2090,  0.0188, -0.3619,  0.1536],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0141, -0.0066, -0.0595,  ...,  0.2832,  0.2405,  0.1628],\n",
       "         [ 0.0412,  0.0332, -0.1390,  ...,  0.2835,  0.1863,  0.1677]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.5739, 0.5261], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(policy_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the diagram that illustrates the overall resulting data flow.\n",
    "\n",
    ".. figure:: /_static/img/reinforcement_learning_diagram.jpg\n",
    "\n",
    "Actions are chosen either randomly or based on a policy, getting the next\n",
    "step sample from the gym environment. We record the results in the\n",
    "replay memory and also run optimization step on every iteration.\n",
    "Optimization picks a random batch from the replay memory to do training of the\n",
    "new policy. \"Older\" target_net is also used in optimization to compute the\n",
    "expected Q values; it is updated occasionally to keep it current.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
