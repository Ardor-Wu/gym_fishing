"""
!git clone https://github.com/openai/spinningup.git
!cd spinningup
!pip install -e ./spinningup/

"""

import spinningup

import gym
import numpy as np
import pandas as pd
import sys
import tensorflow as tf
from collections import defaultdict

from tensorflow import keras
from tensorflow.keras import layers
import math
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from sklearn.metrics import mean_squared_error


#Neural Network

def lstm():
  inputs = keras.Input(shape=(env.observation_space.shape))
  tf.keras.utils.normalize(inputs, axis=-1, order=2)
  layer_1 = layers.Dense(64, activation="relu")
  x = layer_1(inputs)
  layer_2 = layers.Dense(32, activation="relu")(x)
  x = layer_2(inputs)
  output = layers.Dense(8, activation='relu') 
  x = output(inputs)
  model = keras.Model(inputs=inputs, outputs=outputs, name=None)
  inputs.reshape(3, 2)
  model.add(layers.LSTM(64))
  model.add(layers.Dense(8))
  output = model.compile(loss=mean_squared_error, optimizer='adam')
  return output


def lstm_actor_critic(x, a, hidden_sizes=(256,256), activation=tf.nn.relu, 
                     output_activation=tf.tanh, action_space=None):
    act_dim = a.shape.as_list()[-1]
    act_limit = action_space.high[0]
    with tf.variable_scope('pi'):
        pi = act_limit * mlp(x, list(hidden_sizes)+[act_dim], activation, output_activation)
    with tf.variable_scope('q1'):
        q1 = tf.squeeze(mlp(tf.concat([x,a], axis=-1), list(hidden_sizes)+[1], activation, None), axis=1)
    with tf.variable_scope('q2'):
        q2 = tf.squeeze(mlp(tf.concat([x,a], axis=-1), list(hidden_sizes)+[1], activation, None), axis=1)
    with tf.variable_scope('q1', reuse=True):
        q1_pi = tf.squeeze(mlp(tf.concat([x,pi], axis=-1), list(hidden_sizes)+[1], activation, None), axis=1)
    return pi, q1, q2, q1_pi
    
#This will already be done when we make the policy or call agent
#model.compile(loss='mean_squared_error', optimizer='adam')


def get_vars(scope):
  return[x for x in tf.global_variables() if scope in x.name]

def count_vars(scope):
  v = get_vars(scope)
  return sum(np.prod(var.shape.as_list()))

from logx import EpochLogger

class ReplayBuffer:

  def __init__(self, obs_dim, act_dim, size):

    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)
    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)
    self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)
    self.rewards_buf = np.zeros(size, dtype=np.float32)
    self.output_buf = np.zeros(sizem dtype=np.float32)
    self.ptr, self.size, self.max_size= 0,0, size

    def store(self, obs, act, rew, next_obs, done):
        self.obs1_buf[self.ptr] = obs
        self.obs2_buf[self.ptr] = next_obs
        self.acts_buf[self.ptr] = act
        self.rewards_buf[self.ptr] = rew
        self.outputs_buf[self.ptr] = done
        self.ptr = (self.ptr+1) % self.max_size
        self.size = min(self.size+1, self.max_size)

    def sample_batch(self, batch_size=32):
        idxs = np.random.randint(0, self.size, size=batch_size)
        return dict(obs1=self.obs1_buf[idxs],
                    obs2=self.obs2_buf[idxs],
                    acts=self.acts_buf[idxs],
                    rews=self.rewards_buf[idxs],
                    done=self.output_buf[idxs])


import core

def td3(env_fn, actor_critic=lstm_actor_critic, ac_kwargs=dict(), seed=0, 
        steps_per_epoch=1000, epochs=300, replay_size=int(1e3), gamma=0.99, 
        polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000, 
        update_after=1000, update_every=50, act_noise=0.1, target_noise=0.2, 
        noise_clip=0.5, policy_delay=2, num_test_episodes=10, max_ep_len=1000, 
        logger_kwargs=dict(), save_freq=1):
  
  #polyak: averaging for target networks
  #pi_lr: learning rate for policy
  #q_lr: learneing rate for Q-networks
  #update_every: updates after every number of steps

  logger = EpochLogger(**logger_kwargs)
  logger.save_config(locals())

  tf.set_random_seed(seed)
  np.random.seed(seed)

#set dimensions
  env = env_fn()
  observation_dim = env.observation_space.shape[0]
  action_dim = env.action_space.shape[0]

  ac_kwargs['action_space'] = env.action_space
  #make all the dimensions the same bound


  #Target policy networks
  with tf.variable_scope('target'):
        pi_targ, _, _, _  = actor_critic(x2_ph, a_ph, **ac_kwargs)
    
  # Target Q networks
  with tf.variable_scope('target', reuse=True):

    #add noise
    epsilon = tf.random_normal(tf.shape(pi_targ), stddev=target_noise)
    epsilon = tf.clip_by_value(epsilon, -noise_clip, noise_clip)
    smoothed = pi_targ + epsilon
    smoothed = tf.clip_by_value(smoothed, -act_limit, act_limit)

    #set target Q values
    _, q1_targ, q2_targ, _ = actor_critic(x2_ph, a2, **ac_kwargs)

  #Experience Buffer
  replay_buffer = ReplayBuffer(observation_dim=observation_dim, action_dim=action_dim, size=replay_size)

  #Add Bellman Backup for stability
  min_q_target = tf.minimum(q1_target, q2_target)
  
  backup = tf.stop_gradient(r_ph + gamma*(1-d_p)*min_q_target)

  #Loss function for TD3
  pi_loss = -tf.reduce_mean(q1_pi)
  q1_loss = tf.reduce_mean((q1-backup)**2)
  q2_loss = tf.reduce.mean((q2-backup)**2)
  q_loss = q1_loss + q2_loss


  #optimizer
  pi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)
  q_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)
  train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars('main/pi'))
  train_q_op = q_optimizer.minimize(q_loss, var_list=get_vars('main/q'))

  sess = tf.Session()
  sess.run(tf.global_variables_initializer())
  sess.run(target_init)

  #save weights
  logger.setup_tf_saver(sess, inputs={'x': x_ph, 'a': a_ph}, outputs={'pi': pi, 'q1': q1, 'q2': q2})

  def step(observation, noise_scale):
  action = sess.run(pi, feed_dict={x_ph: observation.reshape(1, -1)})
  action += noise_scale + np.random.randn(action_dim)
  return np.clip(a, -act_limit, act_limit)

  def sample():
  for i in range(num_test_episodes):
    observation = env.reset()
    while ep_len <= max_ep_len:
      observation, reward, done, _ = env.step(action(observation, 0))
      ep_return += reward
      ep_len += 1
      d = False if ep_len==max_ep_len else d
      replay_buffer.store(observation, action, reward, observation_2, done)
      observation = observation_2
    logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)



from spinup.utils.run_utils import setup_logger_kwargs
logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)


td3()

